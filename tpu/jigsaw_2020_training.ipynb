{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "c4ad26dd-8d2a-43e9-a3ea-0a4342a16e52",
    "_uuid": "07d7bc95-0377-4db3-aa05-2bd6cf989e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 192 \n",
    "DROPOUT = 0.5 # use aggressive dropout\n",
    "BATCH_SIZE = 16 # per TPU core\n",
    "\n",
    "\n",
    "### Different learning rate for transformer and head ###\n",
    "LR_TRANSFORMER = 5e-6\n",
    "LR_HEAD = 1e-3\n",
    "\n",
    "# PRETRAINED_TOKENIZER=  'jplu/tf-xlm-roberta-large'\n",
    "PRETRAINED_TOKENIZER = '/kaggle/input/jplu-tf-xlm-roberta-large'\n",
    "PRETRAINED_MODEL = '/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large'\n",
    "# PRETRAINED_MODEL = '/kaggle/input/jplu-tf-xlm-roberta-large'\n",
    "D = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "D_TRANS = '/kaggle/input/jigsaw-train-multilingual-coments-google-api/'\n",
    "\n",
    "LANGS = {\n",
    "    'en': 'english',\n",
    "    'it': 'italian', \n",
    "    'fr': 'french', \n",
    "    'es': 'spanish',\n",
    "    'tr': 'turkish', \n",
    "    'ru': 'russian',\n",
    "    'pt': 'portuguese'\n",
    "}\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import transformers\n",
    "from transformers import TFRobertaModel, AutoTokenizer\n",
    "import logging\n",
    "# no extensive logging \n",
    "logging.getLogger().setLevel(logging.NOTSET)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(2020)\n",
    "np.random.seed(2020)\n",
    "rn.seed(2020)\n",
    "tf.random.set_seed(2020)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n",
      "/kaggle/input/jplu-tf-xlm-roberta-large/sentencepiece.bpe.model\n",
      "/kaggle/input/jplu-tf-xlm-roberta-large/config.json\n",
      "/kaggle/input/jplu-tf-xlm-roberta-large/tf_model.h5\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr-cleaned.csv\n",
      "/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large/config.json\n",
      "/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large/tf_model.h5\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dda6b430-ea9c-4772-8bd1-0938cfd6e63b",
    "_uuid": "f0c9b5d5-e529-441d-8f37-7f64a1daccb9"
   },
   "source": [
    "## Connect to TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "86a303dd-5d1d-4d3f-bf28-4a02b6816e9b",
    "_uuid": "5a5fd45e-8f97-44d8-95f6-29cc8563dc3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "def connect_to_TPU():\n",
    "    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n",
    "    try:\n",
    "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "        # set: this is always the case on Kaggle.\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    else:\n",
    "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "    return tpu, strategy, global_batch_size\n",
    "\n",
    "\n",
    "tpu, strategy, global_batch_size = connect_to_TPU()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train1_df = pd.read_csv(D+'jigsaw-toxic-comment-train.csv')\n",
    "# train1_df = train1_df.sort_values(by='id').reset_index(drop=True)\n",
    "# train1_df.columns,train1_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_df = pd.read_csv(D_TRANS+'jigsaw-toxic-comment-train-google-fr.csv')\n",
    "# t1_df = t1_df.sort_values(by='id').reset_index()\n",
    "# t_t_df.toxic = t1_df.toxic.apply(lambda x:eval(x))\n",
    "# t1_df.toxic.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d070fa12-8165-41c2-9828-9690c8efa826",
    "_uuid": "5e6d306f-f2ca-4452-9ace-661e6df8bae8"
   },
   "source": [
    "# tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "900f2d01-b87c-42e7-8cc2-8dcba95ffa54",
    "_uuid": "7d5605a3-df77-4574-9899-89529d0a6e61"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "def regular_encode(texts, tokenizer, maxlen=256):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0725dd72-142e-4139-8e12-103302d53bc9",
    "_uuid": "13500d69-d19f-4ca0-8958-d4d8af5610e8"
   },
   "source": [
    "# datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "757b2c93-9c07-441d-ba2e-47b8607b3795",
    "_uuid": "8b748024-314d-4256-b67b-0bc71ecac19d"
   },
   "outputs": [],
   "source": [
    "def create_dist_dataset(X, y=None, training=False):\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "    ### Add y if present ###\n",
    "    if y is not None:\n",
    "        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n",
    "        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n",
    "        \n",
    "    ### Repeat if training ###\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(len(X)).repeat()\n",
    "    \n",
    "    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n",
    "#     print(len(list(dataset.as_numpy_iterator())))\n",
    "    ### make it distributed  ###\n",
    "    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    return dist_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "3ba0b8c0-3e4c-4afe-8766-844c6b6a8c0f",
    "_uuid": "9f6161c2-a256-4285-9a0e-f23700f4b195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 2 µs, total: 9 µs\n",
      "Wall time: 15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n",
    "def create_model_and_optimizer():\n",
    "    with strategy.scope():\n",
    "        transformer_layer = TFRobertaModel.from_pretrained(PRETRAINED_MODEL)                \n",
    "        model = build_model(transformer_layer)\n",
    "        optimizer_transformer = Adam(learning_rate=LR_TRANSFORMER)\n",
    "        optimizer_head = Adam(learning_rate=LR_HEAD)\n",
    "    return model, optimizer_transformer, optimizer_head\n",
    "\n",
    "\n",
    "def build_model(transformer):\n",
    "    inp = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    # Huggingface transformers have multiple outputs, embeddings are the first one\n",
    "    # let's slice out the first position, the paper says its not worse than pooling\n",
    "    x = transformer(inp)[0][:, 0, :]  \n",
    "#     logits = torch.mean(torch.stack([   # torch.Size([7, 30])\n",
    "#             self.classifier(self.high_dropout(cls_output))\n",
    "#             for _ in range(5)\n",
    "#         ], dim=0), dim=0)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    ### note, adding the name to later identify these weights for different LR\n",
    "    out = Dense(1, activation='sigmoid', name='custom_head')(x)\n",
    "    \n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model, optimizer_transformer, optimizer_head = create_model_and_optimizer()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_linear_schedule_with_warmup(num_warmup_steps, num_training_steps):\n",
    "    \n",
    "    def lrfn(current_step,lr):\n",
    "        if current_step < num_warmup_steps:\n",
    "            lr_mul = (float(current_step)+0.0000001) / float(max(1, num_warmup_steps))\n",
    "        else:\n",
    "            lr_mul = max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
    "        return lr*lr_mul\n",
    "    \n",
    "    return lrfn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_lrfn = get_linear_schedule_with_warmup(30,150)\n",
    "plt.plot([i for i in range(150)], [_lrfn(i,3e-5) for i in range(150)]);\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss & metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_losses_and_metrics():\n",
    "    with strategy.scope():\n",
    "        loss_object = tf.keras.losses.BinaryCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE, from_logits=False)\n",
    "\n",
    "        def compute_loss(labels, predictions):\n",
    "            per_example_loss = loss_object(labels, predictions)\n",
    "            loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss, global_batch_size = global_batch_size)\n",
    "            return loss\n",
    "\n",
    "        train_accuracy_metric = tf.keras.metrics.AUC(name='training_AUC')\n",
    "\n",
    "    return compute_loss, train_accuracy_metric\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0e02e579-4010-4a52-b4ba-f7157b5c9f42",
    "_uuid": "9ce1d72f-d16c-4cde-9edc-d63926623155"
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "20b20e6a-16c4-475f-92ef-2a7809d21621",
    "_uuid": "50c766ac-68bd-4884-ba54-657f5d385786"
   },
   "outputs": [],
   "source": [
    "class train:\n",
    "    def __init__(self,\n",
    "                 train_dist_dataset=None, \n",
    "                 val_dist_dataset=None, \n",
    "                 y_val=None,\n",
    "                 total_steps=2000, \n",
    "                 validate_every=200,\n",
    "                 model=None, \n",
    "                 optimizer_transformer=None,\n",
    "                 optimizer_head=None, \n",
    "                 compute_loss=None,\n",
    "                 train_accuracy_metric=None,\n",
    "                 dir_name='transformer_ptm'):\n",
    "\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer_transformer = optimizer_transformer\n",
    "        self.optimizer_head = optimizer_head\n",
    "        self.compute_loss = compute_loss\n",
    "        self.train_accuracy_metric = train_accuracy_metric \n",
    "        self.dir_name = dir_name\n",
    "        \n",
    "        self.do_train(train_dist_dataset, val_dist_dataset, y_val, total_steps, validate_every)\n",
    "        \n",
    "    def do_train(self,\n",
    "                 train_dist_dataset,\n",
    "                 val_dist_dataset,\n",
    "                 y_val=None,\n",
    "                 total_steps=2000,\n",
    "                 validate_every=200):\n",
    "        \n",
    "        best_weights, history = None, []\n",
    "        scheduler = get_linear_schedule_with_warmup(total_steps//3,total_steps)\n",
    "        step = 0\n",
    "        \n",
    "        for tensor in tqdm(train_dist_dataset, desc=\"training\"):\n",
    "            K.set_value(self.optimizer_transformer.lr, \n",
    "                        scheduler(step,float(LR_TRANSFORMER)))\n",
    "            K.set_value(self.optimizer_head.lr, \n",
    "                        scheduler(step,float(LR_HEAD)))\n",
    "\n",
    "            self.distributed_train_step(tensor) \n",
    "            step+=1\n",
    "\n",
    "\n",
    "            if (step % validate_every == 0):   \n",
    "                train_metric = self.train_accuracy_metric.result().numpy()\n",
    "\n",
    "                val_metric = roc_auc_score(y_val, self.predict(val_dist_dataset))\n",
    "\n",
    "                print(\"Step {}, train AUC: {:.5f}  val AUC: {:.5f}\".format(step, train_metric, val_metric)) \n",
    "\n",
    "                history.append(val_metric)\n",
    "                if history[-1] == max(history):\n",
    "                    best_weights = self.model.get_weights()\n",
    "                    print('best model...')\n",
    "\n",
    "                self.train_accuracy_metric.reset_states()\n",
    "#             if self.step%10==0:\n",
    "#                 print(self.step)\n",
    "            \n",
    "            if step  == total_steps:\n",
    "                break\n",
    "\n",
    "        self.model.set_weights(best_weights)\n",
    "        \n",
    "#         save_model(self.model, self.dir_name)\n",
    "        \n",
    "#         print('save done')\n",
    "\n",
    "    @tf.function\n",
    "    def distributed_train_step(self, data):\n",
    "        strategy.experimental_run_v2(self.train_step, args=(data,))\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        features, labels = inputs\n",
    "\n",
    "        transformer_trainable_variables = [ v for v in self.model.trainable_variables \n",
    "                                           if (('pooler' not in v.name)  and \n",
    "                                               ('custom' not in v.name))]\n",
    "        head_trainable_variables = [ v for v in self.model.trainable_variables \n",
    "                                    if 'custom'  in v.name]\n",
    "\n",
    "        # calculate the 2 gradients ( note persistent, and del)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            predictions = self.model(features, training=True)\n",
    "            loss = self.compute_loss(labels, predictions)\n",
    "\n",
    "            self.optimizer_transformer.apply_gradients(zip(tape.gradient(loss, transformer_trainable_variables), \n",
    "                                                      transformer_trainable_variables))\n",
    "            self.optimizer_head.apply_gradients(zip(tape.gradient(loss, head_trainable_variables), \n",
    "                                               head_trainable_variables))\n",
    "        del tape\n",
    "        del transformer_trainable_variables,head_trainable_variables\n",
    "\n",
    "        self.train_accuracy_metric.update_state(labels, predictions)\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    def predict(self,dataset):  \n",
    "        predictions = []\n",
    "        for tensor in dataset:\n",
    "            predictions.append(self.distributed_prediction_step(tensor))\n",
    "        ### stack replicas and batches\n",
    "        predictions = np.vstack(list(map(np.vstack,predictions)))\n",
    "        return predictions\n",
    "\n",
    "    @tf.function\n",
    "    def distributed_prediction_step(self,data):\n",
    "        predictions = strategy.experimental_run_v2(self.prediction_step, args=(data,))\n",
    "        return strategy.experimental_local_results(predictions)\n",
    "\n",
    "    def prediction_step(self,inputs):\n",
    "        features = inputs  # note datasets used in prediction do not have labels\n",
    "        predictions = self.model(features, training=False)\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "def save_model(model, transformer_dir='transformer_ptm'):\n",
    "    \"\"\"\n",
    "    Special function to load a keras model that uses a transformer layer\n",
    "    \"\"\"\n",
    "    transformer = model.layers[1]\n",
    "    if not os.path.exists(transformer_dir):\n",
    "        os.makedirs(transformer_dir)\n",
    "    transformer.save_pretrained(transformer_dir)\n",
    "    custom_head = model.get_layer('custom_head').get_weights()\n",
    "    pickle.dump(custom_head, open(os.path.join(transformer_dir,'custom_head.pickle'), 'wb'))\n",
    "\n",
    "def load_model(transformer_dir='transformer_ptm'):\n",
    "    \"\"\"\n",
    "    Special function to load a keras model that uses a transformer layer\n",
    "    \"\"\"\n",
    "    transformer = TFRobertaModel.from_pretrained(transformer_dir)\n",
    "    model = build_model(transformer)\n",
    "    custom_head = pickle.load(open(os.path.join(transformer_dir,'custom_head.pickle'), 'rb'))\n",
    "    model.get_layer('custom_head').set_weights(custom_head)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer_transformer, optimizer_head = create_model_and_optimizer()\n",
    "# save_model(model, transformer_dir='transformer_ptm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stage 1 :  data distribution for generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((85536, 3), (8000, 4), (63812, 3), (63812, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_jigsaw_trans(langs=['tr','it','es','ru','fr','pt'], \n",
    "                      columns=['comment_text', 'toxic','id']):\n",
    "    train_6langs=[]\n",
    "    \n",
    "    train1_df = pd.read_csv(D+'jigsaw-toxic-comment-train.csv')\n",
    "    train_6langs.append(train1_df.sort_values(by='id').reset_index(drop=True)[columns])\n",
    "\n",
    "    for i in range(len(langs)):\n",
    "\n",
    "        fn = D_TRANS+'jigsaw-toxic-comment-train-google-%s.csv'%langs[i]\n",
    "        t_t_df = pd.read_csv(fn).sort_values(by='id').reset_index(drop=True)\n",
    "        t_t_df.toxic = t_t_df.toxic.apply(lambda x:eval(x))\n",
    "        train_6langs.append(t_t_df[columns])\n",
    "    \n",
    "    perm_mat = np.zeros((223549,7))\n",
    "    for i in range(223549):\n",
    "        perm_mat[i,:] = np.random.permutation(7)\n",
    "        \n",
    "    train_fold_7 = []\n",
    "    for fold in range(7):\n",
    "        fold_df = []\n",
    "        for idx_df,df in enumerate(train_6langs):\n",
    "            fold_df.append(df.loc[[idx for idx,choice in enumerate(perm_mat[:,fold]) if choice==idx_df]].copy())\n",
    "        fold_df = pd.concat(fold_df).sort_values(by='id').reset_index(drop=True)\n",
    "        \n",
    "        fold_df[~fold_df.toxic.apply(lambda x:isinstance(x,int))] = train_6langs[0][~fold_df.toxic.apply(lambda x:isinstance(x,int))]\n",
    "        \n",
    "        concat_fold_df = [downsample(fold_df.sample(frac=1.0)) for _ in range(2)]\n",
    "\n",
    "    \n",
    "        train_fold_7.append(pd.concat(concat_fold_df))\n",
    "        \n",
    "#     train_6langs.append(pseudo_df[columns])\n",
    "    \n",
    "    return train_fold_7  #pd.concat(train_6langs)#.sample(frac=1.0,random_state=2048)\n",
    "\n",
    "def downsample(df):\n",
    "    \"\"\"Subsample the train dataframe to 50%-50%\"\"\"\n",
    "    ds_df= pd.concat([\n",
    "        df.query('toxic>=0.5'),\n",
    "        df.query('toxic<0.5').sample(df.query('toxic>=0.5').shape[0])\n",
    "    ])\n",
    "    \n",
    "    return ds_df.sample(frac=1.0)\n",
    "    \n",
    "\n",
    "train_7fold_df = load_jigsaw_trans()\n",
    "val_df = pd.read_csv(D+'validation.csv')\n",
    "test_df = pd.read_csv(D+'test.csv')\n",
    "sub_df = pd.read_csv(D+'sample_submission.csv')\n",
    "train_7fold_df[0].shape,val_df.shape,test_df.shape,sub_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_s2.shape, y_val_s2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage2 shape 62\n",
      "train dataset shape 668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 0it [00:00, ?it/s]/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "training: 199it [03:25,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200, train AUC: 0.67821  val AUC: 0.90613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 200it [04:08, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 399it [05:27,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400, train AUC: 0.94877  val AUC: 0.94380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 400it [05:43,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 599it [07:03,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600, train AUC: 0.96596  val AUC: 0.94620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 600it [07:18,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 799it [08:47,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 800, train AUC: 0.96895  val AUC: 0.94616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training: 9it [00:03,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, train AUC: 0.94791  val AUC: 0.94664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 10it [00:45, 12.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 19it [00:48,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20, train AUC: 0.92249  val AUC: 0.94793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 20it [01:04,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 29it [01:07,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30, train AUC: 0.92770  val AUC: 0.95152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 30it [01:23,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 39it [01:26,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40, train AUC: 0.94216  val AUC: 0.95323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 40it [01:41,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 49it [01:44,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50, train AUC: 0.92980  val AUC: 0.95555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 50it [01:59,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 59it [02:03,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60, train AUC: 0.93844  val AUC: 0.95772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 60it [02:18,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 69it [02:21,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 70, train AUC: 0.95322  val AUC: 0.95861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 70it [02:38,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 79it [02:42,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 80, train AUC: 0.94256  val AUC: 0.95895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 80it [03:00,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 89it [03:04,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 90, train AUC: 0.94779  val AUC: 0.95946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 90it [03:19,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 99it [03:23,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, train AUC: 0.95130  val AUC: 0.95956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 99it [03:38,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset shape 668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 0it [00:00, ?it/s]/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "training: 199it [03:24,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200, train AUC: 0.69737  val AUC: 0.93483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 200it [04:06, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 399it [05:26,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400, train AUC: 0.95086  val AUC: 0.94343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 400it [05:41,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 599it [07:01,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600, train AUC: 0.96346  val AUC: 0.94350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 600it [07:16,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 799it [08:36,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 800, train AUC: 0.96820  val AUC: 0.94502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 799it [08:52,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training: 9it [00:03,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, train AUC: 0.92976  val AUC: 0.94570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 10it [00:46, 13.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 19it [00:49,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20, train AUC: 0.92463  val AUC: 0.94675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 20it [01:05,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 29it [01:08,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30, train AUC: 0.93741  val AUC: 0.95033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 30it [01:24,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 39it [01:27,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40, train AUC: 0.93045  val AUC: 0.95185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 40it [01:43,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 50it [01:56,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50, train AUC: 0.93128  val AUC: 0.95160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 59it [02:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60, train AUC: 0.94660  val AUC: 0.95485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 60it [02:15,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 69it [02:18,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 70, train AUC: 0.95406  val AUC: 0.95603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 70it [02:34,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 79it [02:37,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 80, train AUC: 0.95020  val AUC: 0.95638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 80it [02:52,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 89it [02:55,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 90, train AUC: 0.94318  val AUC: 0.95705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 90it [03:11,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 99it [03:14,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, train AUC: 0.94189  val AUC: 0.95709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 99it [03:30,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset shape 668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 0it [00:00, ?it/s]/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "training: 199it [03:23,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200, train AUC: 0.62247  val AUC: 0.87188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 200it [04:05, 12.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 399it [05:24,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400, train AUC: 0.93954  val AUC: 0.94255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 400it [05:39,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 599it [06:59,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600, train AUC: 0.96480  val AUC: 0.94337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 600it [07:14,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 799it [08:44,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 800, train AUC: 0.96764  val AUC: 0.94285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training: 9it [00:03,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, train AUC: 0.92036  val AUC: 0.94467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 10it [00:45, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 19it [00:48,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20, train AUC: 0.92643  val AUC: 0.94501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 20it [01:04,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 29it [01:07,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30, train AUC: 0.92053  val AUC: 0.94807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 30it [01:22,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 39it [01:25,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40, train AUC: 0.92992  val AUC: 0.95005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 40it [01:40,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 49it [01:44,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50, train AUC: 0.93955  val AUC: 0.95314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 50it [01:59,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 59it [02:02,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60, train AUC: 0.93695  val AUC: 0.95430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 60it [02:17,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 69it [02:21,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 70, train AUC: 0.94185  val AUC: 0.95626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 70it [02:36,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 79it [02:39,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 80, train AUC: 0.95043  val AUC: 0.95717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 80it [02:55,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 89it [02:58,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 90, train AUC: 0.93523  val AUC: 0.95772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 90it [03:13,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 99it [03:16,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, train AUC: 0.94355  val AUC: 0.95794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 99it [03:32,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 8s, sys: 6min 39s, total: 26min 48s\n",
      "Wall time: 54min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TOTAL_STEPS_STAGE1 = 800\n",
    "VALIDATE_EVERY_STAGE1 = 200\n",
    "TOTAL_STEPS_STAGE2 = 200\n",
    "VALIDATE_EVERY_STAGE2 = 10\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TOKENIZER)\n",
    "X_val = regular_encode(val_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_test = regular_encode(test_df.content.values, tokenizer, maxlen=MAX_LEN)\n",
    "y_val = val_df.toxic.values.reshape(-1,1).astype('int64')\n",
    "\n",
    "\n",
    "# s2 data\n",
    "X_train_s2 = regular_encode(val_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "y_train_s2 = val_df.toxic.values.reshape(-1,1)\n",
    "\n",
    "val_s2_df = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv')\n",
    "val_s2_df = val_s2_df.sample(len(X_train_s2),random_state=2020)\n",
    "\n",
    "X_val_s2 = regular_encode(val_s2_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "y_val_s2 = val_s2_df.toxic.values.reshape(-1,1)\n",
    "\n",
    "print('stage2 shape {}'.format(len(X_train_s2)//global_batch_size))\n",
    "gc.collect()\n",
    "\n",
    "# Train_class_list = []\n",
    "\n",
    "preds = []\n",
    "\n",
    "for fold in range(4,7):\n",
    "    K.clear_session()\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    \n",
    "    X_train = regular_encode(train_7fold_df[fold].comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "    y_train = train_7fold_df[fold].toxic.values.reshape(-1,1).astype('int64')\n",
    "    \n",
    "    train_dist_dataset = create_dist_dataset(X_train, y_train, True)\n",
    "    val_dist_dataset   = create_dist_dataset(X_val)\n",
    "    test_dist_dataset  = create_dist_dataset(X_test)\n",
    "\n",
    "    print('train dataset shape {}'.format(len(X_train)//global_batch_size))\n",
    "\n",
    "    model, optimizer_transformer, optimizer_head = create_model_and_optimizer()\n",
    "    compute_loss, train_accuracy_metric = define_losses_and_metrics()\n",
    "\n",
    "    Train_class = train(train_dist_dataset=train_dist_dataset, \n",
    "                      val_dist_dataset=val_dist_dataset, \n",
    "                      y_val=y_val,\n",
    "                      total_steps=TOTAL_STEPS_STAGE1, \n",
    "                      validate_every=VALIDATE_EVERY_STAGE1,\n",
    "                      model=model, \n",
    "                      optimizer_transformer=optimizer_transformer, \n",
    "                      optimizer_head=optimizer_head, \n",
    "                      compute_loss=compute_loss, \n",
    "                      train_accuracy_metric=train_accuracy_metric,\n",
    "                      dir_name=f'transformer_ptm_fold_{fold}',\n",
    "                     )\n",
    "    # stage2 --\n",
    "    TOTAL_STEPS_STAGE2 = 100\n",
    "    VALIDATE_EVERY_STAGE2 = 10\n",
    "\n",
    "    # make a datasets\n",
    "    train_dist_dataset_s2 = create_dist_dataset(X_train_s2, y_train_s2, training=True)\n",
    "    train_dist_dataset_s21 = create_dist_dataset(X_train_s2, y_train_s2)\n",
    "    val_dist_dataset_s2 = create_dist_dataset(X_val_s2, y_val_s2)\n",
    "\n",
    "    # train again\n",
    "    Train_class.do_train(train_dist_dataset_s2, train_dist_dataset_s21, y_train_s2,\n",
    "                          total_steps = TOTAL_STEPS_STAGE2, \n",
    "                          validate_every = VALIDATE_EVERY_STAGE2)\n",
    "    \n",
    "    \n",
    "    preds.append(Train_class.predict(test_dist_dataset)[:,0])\n",
    "    \n",
    "    \n",
    "#     val_df['toxic'] = Train_class.predict(val_dist_dataset)[:,0]\n",
    "#     test_df['toxic'] = Train_class.predict(test_dist_dataset)[:,0]\n",
    "#     val_df.to_csv(f'pseudo_val_fold{fold}.csv', index=False);\n",
    "#     test_df.to_csv(f'pseudo_test_fold{fold}.csv', index=False);\n",
    "    del X_train,y_train,train_dist_dataset,val_dist_dataset,test_dist_dataset\n",
    "    del model,optimizer_transformer,optimizer_head,compute_loss,train_accuracy_metric\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stage 2 : pseudo label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# val_df['toxic'] = predict(val_dist_dataset)[:,0]\n",
    "# test_df['toxic'] = predict(test_dist_dataset)[:,0]\n",
    "# val_df.to_csv('pseudo_val.csv', index=False);\n",
    "# test_df.to_csv('pseudo_test.csv', index=False);\n",
    "# val_df.columns,sub_df.columns,test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo_test_df = pd.read_csv('/kaggle/input/jigsaw-tpu2-dataset/pseudo_test_fold1.csv')\n",
    "# t_t2 = pd.read_csv('/kaggle/input/jigsaw-tpu2-dataset/pseudo_test_fold0.csv')['toxic']\n",
    "# pseudo_test_df['toxic'] = (pseudo_test_df['toxic']+t_t2)/2\n",
    "# pseudo_test_df['comment_text'] = pseudo_test_df['content']\n",
    "\n",
    "# pseudo_val_df = pd.read_csv('/kaggle/input/jigsaw-tpu2-dataset/pseudo_val_fold1.csv')\n",
    "# t_t2 = pd.read_csv('/kaggle/input/jigsaw-tpu2-dataset/pseudo_val_fold0.csv')['toxic']\n",
    "# pseudo_val_df['toxic'] = (pseudo_val_df['toxic']+t_t2)/2\n",
    "\n",
    "# # /kaggle/input/jigsaw-tpu2/transformer_ptm_fold_0/tf_model.h5\n",
    "# # /kaggle/input/jigsaw-tpu2/transformer_ptm_fold_0/config.json\n",
    "# # /kaggle/input/jigsaw-tpu2/transformer_ptm_fold_0/custom_head.pickle\n",
    "# # /kaggle/input/jigsaw-tpu2/transformer_ptm_fold_1/tf_model.h5\n",
    "# # /kaggle/input/jigsaw-tpu2/transformer_ptm_fold_1/config.json\n",
    "# # /kaggle/input/jigsaw-tpu2/transformer_ptm_fold_1/custom_head.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_jigsaw_trans(langs=['tr','it','es','ru','fr','pt'], \n",
    "#                       columns=['comment_text', 'toxic','id']):\n",
    "#     train_6langs=[]\n",
    "    \n",
    "#     train1_df = pd.read_csv(D+'jigsaw-toxic-comment-train.csv')\n",
    "#     train_6langs.append(train1_df.sort_values(by='id').reset_index(drop=True)[columns])\n",
    "\n",
    "#     for i in range(len(langs)):\n",
    "\n",
    "#         fn = D_TRANS+'jigsaw-toxic-comment-train-google-%s.csv'%langs[i]\n",
    "#         t_t_df = pd.read_csv(fn).sort_values(by='id').reset_index(drop=True)\n",
    "#         t_t_df.toxic = t_t_df.toxic.apply(lambda x:eval(x))\n",
    "#         train_6langs.append(t_t_df[columns])\n",
    "    \n",
    "#     perm_mat = np.zeros((223549,7))\n",
    "#     for i in range(223549):\n",
    "#         perm_mat[i,:] = np.random.permutation(7)\n",
    "        \n",
    "#     train_fold_7 = []\n",
    "#     for fold in range(7):\n",
    "#         fold_df = []\n",
    "#         for idx_df,df in enumerate(train_6langs):\n",
    "#             fold_df.append(df.loc[[idx for idx,choice in enumerate(perm_mat[:,fold]) if choice==idx_df]].copy())\n",
    "#         fold_df = pd.concat(fold_df).sort_values(by='id').reset_index(drop=True)\n",
    "        \n",
    "#         fold_df[~fold_df.toxic.apply(lambda x:isinstance(x,int))] = train_6langs[0][~fold_df.toxic.apply(lambda x:isinstance(x,int))]\n",
    "        \n",
    "#         concat_fold_df = [downsample(fold_df.sample(frac=1.0)) for _ in range(2)]\n",
    "        \n",
    "# #         pseudo_path = f'/kaggle/input/jigsaw-tpu2/pseudo_test_fold{fold}.csv'\n",
    "# #         if os.path.exists(pseudo_path):\n",
    "# #             pseudo_test_df = pd.read_csv(f'/kaggle/input/jigsaw-tpu2/pseudo_test_fold{fold}.csv')\n",
    "# #             pseudo_val_df = pd.read_csv(f'/kaggle/input/jigsaw-tpu2/pseudo_val_fold{fold}.csv')\n",
    "# #             pseudo_test_df['comment_text'] = pseudo_test_df['content']\n",
    "        \n",
    "# #             concat_fold_df.append(downsample(pseudo_val_df.sample(frac=1.0)))\n",
    "# #             concat_fold_df.append(downsample(pseudo_test_df.sample(frac=1.0)))\n",
    "        \n",
    "#         concat_fold_df.append(downsample(pseudo_val_df.sample(frac=1.0)))\n",
    "#         concat_fold_df.append(downsample(pseudo_test_df.sample(frac=0.8)))\n",
    "        \n",
    "#         train_fold_7.append(pd.concat(concat_fold_df).sample(frac=1.0))\n",
    "        \n",
    "# #     train_6langs.append(pseudo_df[columns])\n",
    "    \n",
    "#     return train_fold_7  #pd.concat(train_6langs)#.sample(frac=1.0,random_state=2048)\n",
    "\n",
    "# def downsample(df):\n",
    "#     \"\"\"Subsample the train dataframe to 50%-50%\"\"\"\n",
    "#     ds_df= pd.concat([\n",
    "#         df.query('toxic>=0.5'),\n",
    "#         df.query('toxic<0.5').sample(df.query('toxic>=0.5').shape[0])\n",
    "#     ])\n",
    "    \n",
    "#     return ds_df.sample(frac=1.0)\n",
    "    \n",
    "\n",
    "# train_7fold_df = load_jigsaw_trans()\n",
    "# val_df = pd.read_csv(D+'validation.csv')\n",
    "# test_df = pd.read_csv(D+'test.csv')\n",
    "# sub_df = pd.read_csv(D+'sample_submission.csv')\n",
    "# train_7fold_df[0].shape,val_df.shape,test_df.shape,sub_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# TOTAL_STEPS_STAGE1 = 800\n",
    "# VALIDATE_EVERY_STAGE1 = 200\n",
    "# TOTAL_STEPS_STAGE2 = 200\n",
    "# VALIDATE_EVERY_STAGE2 = 10\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TOKENIZER)\n",
    "# X_val = regular_encode(val_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "# X_test = regular_encode(test_df.content.values, tokenizer, maxlen=MAX_LEN)\n",
    "# y_val = val_df.toxic.values.reshape(-1,1).astype('int64')\n",
    "\n",
    "# Train_class_list = []\n",
    "# for fold in range(3):\n",
    "#     K.clear_session()\n",
    "#     tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    \n",
    "#     X_train = regular_encode(train_7fold_df[fold].comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "#     y_train = train_7fold_df[fold].toxic.values.reshape(-1,1).astype('int64')\n",
    "    \n",
    "#     train_dist_dataset = create_dist_dataset(X_train, y_train, True)\n",
    "#     val_dist_dataset   = create_dist_dataset(X_val)\n",
    "#     test_dist_dataset  = create_dist_dataset(X_test)\n",
    "\n",
    "#     print('train dataset shape {}'.format(len(X_train)//global_batch_size))\n",
    "\n",
    "    \n",
    "#     model_path = f'/kaggle/input/jigsaw-tpu2/transformer_ptm_fold_{fold}'\n",
    "#     if os.path.exists(model_path):\n",
    "#         with strategy.scope():\n",
    "#             model = load_model(model_path)\n",
    "#             optimizer_transformer = Adam(learning_rate=LR_TRANSFORMER)\n",
    "#             optimizer_head = Adam(learning_rate=LR_HEAD)\n",
    "#     else:\n",
    "#         model, optimizer_transformer, optimizer_head = create_model_and_optimizer()\n",
    "        \n",
    "#     compute_loss, train_accuracy_metric = define_losses_and_metrics()\n",
    "\n",
    "#     Train_class_list.append(\n",
    "#         train(train_dist_dataset=train_dist_dataset, \n",
    "#                       val_dist_dataset=val_dist_dataset, \n",
    "#                       y_val=y_val,\n",
    "#                       total_steps=TOTAL_STEPS_STAGE1, \n",
    "#                       validate_every=VALIDATE_EVERY_STAGE1,\n",
    "#                       model=model, \n",
    "#                       optimizer_transformer=optimizer_transformer, \n",
    "#                       optimizer_head=optimizer_head, \n",
    "#                       compute_loss=compute_loss, \n",
    "#                       train_accuracy_metric=train_accuracy_metric,\n",
    "#                       dir_name=f'transformer_ptm_fold_{fold}',\n",
    "#                      ))\n",
    "# #     train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "# #           TOTAL_STEPS_STAGE1, VALIDATE_EVERY_STAGE1, dir_name=f'transformer_ptm_fold_{fold}')\n",
    "    \n",
    "# #     val_df['toxic'] = Train_class.predict(val_dist_dataset)[:,0]\n",
    "# #     test_df['toxic'] = Train_class.predict(test_dist_dataset)[:,0]\n",
    "# #     val_df.to_csv(f'pseudo_val_fold{fold}.csv', index=False);\n",
    "# #     test_df.to_csv(f'pseudo_test_fold{fold}.csv', index=False);\n",
    "#     del X_train, y_train, train_dist_dataset, val_dist_dataset, test_dist_dataset\n",
    "#     gc.collect()\n",
    "# #     del model,optimizer_transformer,optimizer_head,compute_loss,train_accuracy_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a280adb2-4463-4ad4-8f4a-22bc43292d82",
    "_uuid": "803c2d73-e750-4189-aa53-db248fc25ea7"
   },
   "source": [
    "# stage3: finetune in val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTAL_STEPS_STAGE2 = 100\n",
    "# VALIDATE_EVERY_STAGE2 = 10\n",
    "\n",
    "# # make a datasets\n",
    "# train_dist_dataset = create_dist_dataset(X_train_s2, y_train_s2, training=True)\n",
    "# train_dist_dataset1 = create_dist_dataset(X_train_s2, y_train_s2)\n",
    "# val_dist_dataset = create_dist_dataset(X_val_s2, y_val_s2)\n",
    "\n",
    "# # train again\n",
    "# for Train_class in Train_class_list:\n",
    "#     Train_class.do_train(train_dist_dataset, train_dist_dataset1, y_train_s2,\n",
    "#                           total_steps = TOTAL_STEPS_STAGE2, \n",
    "#                           validate_every = VALIDATE_EVERY_STAGE2)  # not validating but printing now\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e8be7da4-a25f-482c-88fa-c9b87a8a5f1e",
    "_uuid": "62874aa9-54a8-495b-a2a9-ac726c6dbecf"
   },
   "source": [
    "## Make predictions and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "96a7be93-0dce-4d00-92c4-d4c4cb2238f8",
    "_uuid": "5cd548e4-5756-4d92-b72c-f71cb72b395d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 340 ms, sys: 32.2 ms, total: 372 ms\n",
      "Wall time: 692 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sub_df['toxic'] = sum(preds)/len(preds)\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
