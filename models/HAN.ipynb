{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Attention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中间的attention架构\n",
    "https://zhuanlan.zhihu.com/p/54165155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, word_rnn_size, word_rnn_layers, word_att_size, dropout):\n",
    "        \"\"\"\n",
    "        :param vocab_size: number of words in the vocabulary of the model\n",
    "        :param emb_size: size of word embeddings\n",
    "        :param word_rnn_size: size of (bidirectional) word-level RNN\n",
    "        :param word_rnn_layers: number of layers in word-level RNN\n",
    "        :param word_att_size: size of word-level attention layer\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(WordAttention, self).__init__()\n",
    "\n",
    "        # Embeddings (look-up) layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "        # Bidirectional word-level RNN\n",
    "        self.word_rnn = nn.GRU(emb_size, word_rnn_size, num_layers=word_rnn_layers, bidirectional=True,\n",
    "                               dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Word-level attention network\n",
    "        self.word_attention = nn.Linear(2 * word_rnn_size, word_att_size)\n",
    "\n",
    "        # Word context vector to take dot-product with\n",
    "        self.word_context_vector = nn.Linear(word_att_size, 1, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, sentences, words_per_sentence):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param sentences: encoded sentence-level data, a tensor of dimension (n_sentences, word_pad_len, emb_size)\n",
    "        :param words_per_sentence: sentence lengths, a tensor of dimension (n_sentences)\n",
    "        :return: sentence embeddings, attention weights of words\n",
    "        \"\"\"\n",
    "        # Sort sentences by decreasing sentence lengths (SORTING #2)\n",
    "        words_per_sentence, sent_sort_ind = words_per_sentence.sort(dim=0, descending=True)\n",
    "        sentences = sentences[sent_sort_ind]  # (n_sentences, word_pad_len, emb_size)\n",
    "\n",
    "        # Get word embeddings, apply dropout\n",
    "        sentences = self.dropout(self.embeddings(sentences))  # (n_sentences, word_pad_len, emb_size)\n",
    "\n",
    "        # Re-arrange as words by removing pad-words (SENTENCES -> WORDS)\n",
    "        words, bw = pack_padded_sequence(sentences,\n",
    "                                         lengths=words_per_sentence.tolist(),\n",
    "                                         batch_first=True)\n",
    "        # (n_words, emb_size), bw is the effective batch size at each word-timestep\n",
    "\n",
    "        (words, _), _ = self.word_rnn(PackedSequence(words, bw))  # (n_words, 2 * word_rnn_size), (max(sent_lens))\n",
    "\n",
    "        # Find attention vectors by applying the attention linear layer\n",
    "        att_w = self.word_attention(words)  # (n_words, att_size)\n",
    "        att_w = F.tanh(att_w)  # (n_words, att_size)\n",
    "\n",
    "        # Take the dot-product of the attention vectors with the context vector (i.e. parameter of linear layer)\n",
    "        att_w = self.word_context_vector(att_w).squeeze(1)  # (n_words)\n",
    "\n",
    "        max_value = att_w.max()  \n",
    "        att_w = torch.exp(att_w - max_value)  # (n_words)\n",
    "\n",
    "        # Re-arrange as sentences by re-padding with 0s (WORDS -> SENTENCES)\n",
    "        att_w, _ = pad_packed_sequence(PackedSequence(att_w, bw), batch_first=True)\n",
    "        # (n_sentences, max_sent_len_in_batch)\n",
    "\n",
    "        # Calculate softmax values\n",
    "        word_alphas = att_w / torch.sum(att_w, dim=1, keepdim=True)\n",
    "\n",
    "        # (n_sentences, max_sent_len_in_batch)\n",
    "\n",
    "        # Similarly re-arrange word-level RNN outputs as sentences by re-padding with 0s (WORDS -> SENTENCES)\n",
    "        sentences, _ = pad_packed_sequence(PackedSequence(words, bw), batch_first=True)\n",
    "        # (n_sentences, max_sent_len_in_batch, 2 * word_rnn_size)\n",
    "\n",
    "        # Find sentence embeddings\n",
    "        sentences = sentences * word_alphas.unsqueeze(2)  # (n_sentences, max_sent_len_in_batch, 2 * word_rnn_size)\n",
    "        sentences = sentences.sum(dim=1)  # (n_sentences, 2 * word_rnn_size)\n",
    "\n",
    "        # Unsort sentences into the original order (INVERSE OF SORTING #2)\n",
    "        _, sent_unsort_ind = sent_sort_ind.sort(dim=0, descending=False)  # (n_sentences)\n",
    "        sentences = sentences[sent_unsort_ind]  # (n_sentences, 2 * word_rnn_size)\n",
    "        word_alphas = word_alphas[sent_unsort_ind]  # (n_sentences, max_sent_len_in_batch)\n",
    "\n",
    "        return sentences, word_alphas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
