{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectors 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab = torchtext.vocab.Vectors(name='glove.6B.100d.txt',cache='H:\\DBAI\\word_vec\\glove.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = ['chip', 'baby', 'Beautiful']\n",
    "ret = glove_vocab.get_vecs_by_tokens(examples, lower_case_backup=True)\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vocab 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "def read_imdb(folder='train', data_root=r\"H:\\DBAI\\BenchMark_DataSet\\imdb\\aclImdb\"): \n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join(data_root, folder, label)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '').lower()\n",
    "                data.append([review, 1 if label == 'pos' else 0])\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "def get_tokenized_imdb(data):\n",
    "    \"\"\"\n",
    "    data: list of [string, label]\n",
    "    \"\"\"\n",
    "    def tokenizer(text):\n",
    "        text = re.sub('\\.',' . ',text)\n",
    "#         text = re.sub('\\.',' .',text) \n",
    "        text = re.sub('<br />',' ',text) \n",
    "        return [tok.lower() for tok in text.split()]\n",
    "    return [tokenizer(review) for review, _ in data]\n",
    "\n",
    "def get_vocab_imdb(data):\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return torchtext.vocab.Vocab(counter, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac7cbfbd8ea4c5cb244475ea0cf48a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743ce3381dd4411da4f11fe538f6fdbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = read_imdb('train')\n",
    "vocab = get_vocab_imdb(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9276, 1112, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['chip', 'baby', 'Beautiful']\n",
    "[vocab.stoi[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5426,  0.4148,  1.0322, -0.4024,  0.4669,  0.2182, -0.0749,  0.4733,\n",
       "          0.0810, -0.2208, -0.1281, -0.1144,  0.5089,  0.1157,  0.0282, -0.3628,\n",
       "          0.4382,  0.0475,  0.2028,  0.4986, -0.1007,  0.1327,  0.1697,  0.1165,\n",
       "          0.3135,  0.2571,  0.0928, -0.5683, -0.5297, -0.0515, -0.6733,  0.9253,\n",
       "          0.2693,  0.2273,  0.6636,  0.2622,  0.1972,  0.2609,  0.1877, -0.3454,\n",
       "         -0.4263,  0.1398,  0.5634, -0.5691,  0.1240, -0.1289,  0.7248, -0.2610,\n",
       "         -0.2631, -0.4360,  0.0789, -0.8415,  0.5160,  1.3997, -0.7646, -3.1453,\n",
       "         -0.2920, -0.3125,  1.5129,  0.5243,  0.2146,  0.4245, -0.0884, -0.1780,\n",
       "          1.1876,  0.1058,  0.7657,  0.2191,  0.3582, -0.1164,  0.0933, -0.6248,\n",
       "         -0.2190,  0.2180,  0.7406, -0.4374,  0.1434,  0.1472, -1.1605, -0.0505,\n",
       "          0.1268, -0.0144, -0.9868, -0.0913, -1.2054, -0.1197,  0.0478, -0.5400,\n",
       "          0.5246, -0.7096, -0.3253, -0.1346, -0.4131,  0.3343, -0.0072,  0.3225,\n",
       "         -0.0442, -1.2969,  0.7622,  0.4635]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "#             print(word)\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed\n",
    "\n",
    "load_pretrained_embedding(vocab.itos[8:9], glove_vocab)\n",
    "# vocab.itos[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其他一整套原生的使用方式（不打算探究）\n",
    "### Field本来是为了配置数据字段，但被赋予了过多功能\n",
    "### 数据集的相关操作也被封装起来，但我觉得这在应用层面会有相当多的调整，并不适合封装\n",
    "### 数据集的抓取倒是一个不错的资源管道\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "# set up fields\n",
    "TEXT = torchtext.data.Field(lower=True, include_lengths=False, batch_first=True)\n",
    "LABEL = torchtext.data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train, test = torchtext.datasets.IMDB.splits(TEXT, LABEL, root=r'H:\\DBAI\\BenchMark_DataSet')\n",
    "\n",
    "# build the vocabulary\n",
    "TEXT.build_vocab(train, \n",
    "                 vectors=torchtext.vocab.Vectors(name='glove.6B.100d.txt',\n",
    "                                                 cache='H:\\DBAI\\word_vec\\glove.6B'))\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, test), batch_size=3, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 3 from IMDB]\n",
      "\t[.text]:[torch.LongTensor of size 3x629]\n",
      "\t[.label]:[torch.LongTensor of size 3] 0\n",
      "tensor([[   208,     19,   1065,  ...,      1,      1,      1],\n",
      "        [    50,     69,    759,  ...,     18,  20108,   1858],\n",
      "        [204292,     10,      7,  ...,      1,      1,      1]])\n"
     ]
    }
   ],
   "source": [
    "for idx,batch in enumerate(train_iter):\n",
    "    print(batch,idx)\n",
    "    print(batch.text)\n",
    "    if idx==0:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8334"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package torchtext.datasets in torchtext:\n",
      "\n",
      "NAME\n",
      "    torchtext.datasets\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    babi\n",
      "    imdb\n",
      "    language_modeling\n",
      "    nli\n",
      "    sequence_tagging\n",
      "    sst\n",
      "    text_classification\n",
      "    translation\n",
      "    trec\n",
      "    unsupervised_learning\n",
      "\n",
      "CLASSES\n",
      "    torch.utils.data.dataset.Dataset(builtins.object)\n",
      "        torchtext.datasets.text_classification.TextClassificationDataset\n",
      "        torchtext.datasets.unsupervised_learning.EnWik9\n",
      "    torchtext.data.dataset.Dataset(torch.utils.data.dataset.Dataset)\n",
      "        torchtext.datasets.babi.BABI20\n",
      "        torchtext.datasets.imdb.IMDB\n",
      "        torchtext.datasets.language_modeling.LanguageModelingDataset\n",
      "            torchtext.datasets.language_modeling.PennTreebank\n",
      "            torchtext.datasets.language_modeling.WikiText103\n",
      "            torchtext.datasets.language_modeling.WikiText2\n",
      "        torchtext.datasets.sequence_tagging.SequenceTaggingDataset\n",
      "            torchtext.datasets.sequence_tagging.CoNLL2000Chunking\n",
      "            torchtext.datasets.sequence_tagging.UDPOS\n",
      "        torchtext.datasets.sst.SST\n",
      "        torchtext.datasets.translation.TranslationDataset\n",
      "            torchtext.datasets.translation.IWSLT\n",
      "            torchtext.datasets.translation.Multi30k\n",
      "            torchtext.datasets.translation.WMT14\n",
      "        torchtext.datasets.trec.TREC\n",
      "    torchtext.datasets.nli.NLIDataset(torchtext.data.dataset.TabularDataset)\n",
      "        torchtext.datasets.nli.MultiNLI\n",
      "        torchtext.datasets.nli.SNLI\n",
      "        torchtext.datasets.nli.XNLI\n",
      "    \n",
      "    class BABI20(torchtext.data.dataset.Dataset)\n",
      "     |  BABI20(path, text_field, only_supporting=False, **kwargs)\n",
      "     |  \n",
      "     |  Defines a dataset composed of Examples along with its Fields.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      sort_key (callable): A key to use for sorting dataset examples for batching\n",
      "     |          together examples with similar lengths to minimize padding.\n",
      "     |      examples (list(Example)): The examples in this dataset.\n",
      "     |      fields (dict[str, Field]): Contains the name of each column or field, together\n",
      "     |          with the corresponding Field object. Two fields with the same Field object\n",
      "     |          will have a shared vocabulary.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BABI20\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, path, text_field, only_supporting=False, **kwargs)\n",
      "     |      Create a dataset from a list of Examples and Fields.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          examples: List of Examples.\n",
      "     |          fields (List(tuple(str, Field))): The Fields to use in this tuple. The\n",
      "     |              string is a field name, and the Field is the associated field.\n",
      "     |          filter_pred (callable or None): Use only examples for which\n",
      "     |              filter_pred(example) is True, or use all examples if None.\n",
      "     |              Default is None.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  iters(batch_size=32, root='.data', memory_size=50, task=1, joint=False, tenK=False, only_supporting=False, sort=False, shuffle=False, device=None, **kwargs) from builtins.type\n",
      "     |  \n",
      "     |  splits(text_field, path=None, root='.data', task=1, joint=False, tenK=False, only_supporting=False, train=None, validation=None, test=None, **kwargs) from builtins.type\n",
      "     |      Create Dataset objects for multiple splits of a dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path (str): Common prefix of the splits' file paths, or None to use\n",
      "     |              the result of cls.download(root).\n",
      "     |          root (str): Root dataset storage directory. Default is '.data'.\n",
      "     |          train (str): Suffix to add to path for the train set, or None for no\n",
      "     |              train set. Default is None.\n",
      "     |          validation (str): Suffix to add to path for the validation set, or None\n",
      "     |              for no validation set. Default is None.\n",
      "     |          test (str): Suffix to add to path for the test set, or None for no test\n",
      "     |              set. Default is None.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of the\n",
      "     |              Dataset (sub)class being used.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = ''\n",
      "     |  \n",
      "     |  name = ''\n",
      "     |  \n",
      "     |  urls = ['http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2....\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  sort_key = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CoNLL2000Chunking(SequenceTaggingDataset)\n",
      "     |  CoNLL2000Chunking(path, fields, encoding='utf-8', separator='\\t', **kwargs)\n",
      "     |  \n",
      "     |  Defines a dataset for sequence tagging. Examples in this dataset\n",
      "     |  contain paired lists -- paired list of words and tags.\n",
      "     |  \n",
      "     |  For example, in the case of part-of-speech tagging, an example is of the\n",
      "     |  form\n",
      "     |  [I, love, PyTorch, .] paired with [PRON, VERB, PROPN, PUNCT]\n",
      "     |  \n",
      "     |  See torchtext/test/sequence_tagging.py on how to use this class.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CoNLL2000Chunking\n",
      "     |      SequenceTaggingDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  splits(fields, root='.data', train='train.txt', test='test.txt', validation_frac=0.1, **kwargs) from builtins.type\n",
      "     |      Downloads and loads the CoNLL 2000 Chunking dataset.\n",
      "     |      NOTE: There is only a train and test dataset so we use\n",
      "     |            10% of the train set as validation\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = ''\n",
      "     |  \n",
      "     |  name = 'conll2000'\n",
      "     |  \n",
      "     |  urls = ['https://www.clips.uantwerpen.be/conll2000/chunking/train.txt....\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SequenceTaggingDataset:\n",
      "     |  \n",
      "     |  __init__(self, path, fields, encoding='utf-8', separator='\\t', **kwargs)\n",
      "     |      Create a dataset from a list of Examples and Fields.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          examples: List of Examples.\n",
      "     |          fields (List(tuple(str, Field))): The Fields to use in this tuple. The\n",
      "     |              string is a field name, and the Field is the associated field.\n",
      "     |          filter_pred (callable or None): Use only examples for which\n",
      "     |              filter_pred(example) is True, or use all examples if None.\n",
      "     |              Default is None.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from SequenceTaggingDataset:\n",
      "     |  \n",
      "     |  sort_key(example)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class EnWik9(torch.utils.data.dataset.Dataset)\n",
      "     |  EnWik9(begin_line=0, num_lines=6348957, root='.data')\n",
      "     |  \n",
      "     |  Compressed size of first 10^9 bytes of enwiki-20060303-pages-articles.xml.\n",
      "     |  It's part of Large Text Compression Benchmark project\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      EnWik9\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __init__(self, begin_line=0, num_lines=6348957, root='.data')\n",
      "     |      Initiate EnWik9 dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          begin_line: the number of beginning line. Default: 0\n",
      "     |          num_lines: the number of lines to be loaded. Default: 6348957\n",
      "     |          root: Directory where the datasets are saved. Default: \".data\"\n",
      "     |          data: a list of label/tokens tuple. tokens are a tensor after\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |          >>> from torchtext.datasets import EnWik9\n",
      "     |          >>> enwik9 = EnWik9(num_lines=20000)\n",
      "     |          >>> vocab = enwik9.get_vocab()\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  get_vocab(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class IMDB(torchtext.data.dataset.Dataset)\n",
      "     |  IMDB(path, text_field, label_field, **kwargs)\n",
      "     |  \n",
      "     |  Defines a dataset composed of Examples along with its Fields.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      sort_key (callable): A key to use for sorting dataset examples for batching\n",
      "     |          together examples with similar lengths to minimize padding.\n",
      "     |      examples (list(Example)): The examples in this dataset.\n",
      "     |      fields (dict[str, Field]): Contains the name of each column or field, together\n",
      "     |          with the corresponding Field object. Two fields with the same Field object\n",
      "     |          will have a shared vocabulary.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IMDB\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, path, text_field, label_field, **kwargs)\n",
      "     |      Create an IMDB dataset instance given a path and fields.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path: Path to the dataset's highest level directory\n",
      "     |          text_field: The field that will be used for text data.\n",
      "     |          label_field: The field that will be used for label data.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of\n",
      "     |              data.Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  iters(batch_size=32, device=0, root='.data', vectors=None, **kwargs) from builtins.type\n",
      "     |      Create iterator objects for splits of the IMDB dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          batch_size: Batch_size\n",
      "     |          device: Device to create batches on. Use - 1 for CPU and None for\n",
      "     |              the currently active GPU device.\n",
      "     |          root: The root directory that contains the imdb dataset subdirectory\n",
      "     |          vectors: one of the available pretrained vectors or a list with each\n",
      "     |              element one of the available pretrained vectors (see Vocab.load_vectors)\n",
      "     |      \n",
      "     |          Remaining keyword arguments: Passed to the splits method.\n",
      "     |  \n",
      "     |  splits(text_field, label_field, root='.data', train='train', test='test', **kwargs) from builtins.type\n",
      "     |      Create dataset objects for splits of the IMDB dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          text_field: The field that will be used for the sentence.\n",
      "     |          label_field: The field that will be used for label data.\n",
      "     |          root: Root dataset storage directory. Default is '.data'.\n",
      "     |          train: The directory that contains the training examples\n",
      "     |          test: The directory that contains the test examples\n",
      "     |          Remaining keyword arguments: Passed to the splits method of\n",
      "     |              Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  sort_key(ex)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = 'aclImdb'\n",
      "     |  \n",
      "     |  name = 'imdb'\n",
      "     |  \n",
      "     |  urls = ['http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.g...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class IWSLT(TranslationDataset)\n",
      "     |  IWSLT(path, exts, fields, **kwargs)\n",
      "     |  \n",
      "     |  The IWSLT 2016 TED talk translation task\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IWSLT\n",
      "     |      TranslationDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  splits(exts, fields, root='.data', train='train', validation='IWSLT16.TED.tst2013', test='IWSLT16.TED.tst2014', **kwargs) from builtins.type\n",
      "     |      Create dataset objects for splits of the IWSLT dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          exts: A tuple containing the extension to path for each language.\n",
      "     |          fields: A tuple containing the fields that will be used for data\n",
      "     |              in each language.\n",
      "     |          root: Root dataset storage directory. Default is '.data'.\n",
      "     |          train: The prefix of the train data. Default: 'train'.\n",
      "     |          validation: The prefix of the validation data. Default: 'val'.\n",
      "     |          test: The prefix of the test data. Default: 'test'.\n",
      "     |          Remaining keyword arguments: Passed to the splits method of\n",
      "     |              Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  clean(path)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  base_dirname = '{}-{}'\n",
      "     |  \n",
      "     |  base_url = 'https://wit3.fbk.eu/archive/2016-01//texts/{}/{}/{}.tgz'\n",
      "     |  \n",
      "     |  name = 'iwslt'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TranslationDataset:\n",
      "     |  \n",
      "     |  __init__(self, path, exts, fields, **kwargs)\n",
      "     |      Create a TranslationDataset given paths and fields.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path: Common prefix of paths to the data files for both languages.\n",
      "     |          exts: A tuple containing the extension to path for each language.\n",
      "     |          fields: A tuple containing the fields that will be used for data\n",
      "     |              in each language.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of\n",
      "     |              data.Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from TranslationDataset:\n",
      "     |  \n",
      "     |  sort_key(ex)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LanguageModelingDataset(torchtext.data.dataset.Dataset)\n",
      "     |  LanguageModelingDataset(path, text_field, newline_eos=True, encoding='utf-8', **kwargs)\n",
      "     |  \n",
      "     |  Defines a dataset for language modeling.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LanguageModelingDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, path, text_field, newline_eos=True, encoding='utf-8', **kwargs)\n",
      "     |      Create a LanguageModelingDataset given a path and a field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path: Path to the data file.\n",
      "     |          text_field: The field that will be used for text data.\n",
      "     |          newline_eos: Whether to add an <eos> token for every newline in the\n",
      "     |              data file. Default: True.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of\n",
      "     |              data.Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  splits(path=None, root='.data', train=None, validation=None, test=None, **kwargs) from builtins.type\n",
      "     |      Create Dataset objects for multiple splits of a dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path (str): Common prefix of the splits' file paths, or None to use\n",
      "     |              the result of cls.download(root).\n",
      "     |          root (str): Root dataset storage directory. Default is '.data'.\n",
      "     |          train (str): Suffix to add to path for the train set, or None for no\n",
      "     |              train set. Default is None.\n",
      "     |          validation (str): Suffix to add to path for the validation set, or None\n",
      "     |              for no validation set. Default is None.\n",
      "     |          test (str): Suffix to add to path for the test set, or None for no test\n",
      "     |              set. Default is None.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of the\n",
      "     |              Dataset (sub)class being used.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  sort_key = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Multi30k(TranslationDataset)\n",
      "     |  Multi30k(path, exts, fields, **kwargs)\n",
      "     |  \n",
      "     |  The small-dataset WMT 2016 multimodal task, also known as Flickr30k\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Multi30k\n",
      "     |      TranslationDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  splits(exts, fields, root='.data', train='train', validation='val', test='test2016', **kwargs) from builtins.type\n",
      "     |      Create dataset objects for splits of the Multi30k dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          exts: A tuple containing the extension to path for each language.\n",
      "     |          fields: A tuple containing the fields that will be used for data\n",
      "     |              in each language.\n",
      "     |          root: Root dataset storage directory. Default is '.data'.\n",
      "     |          train: The prefix of the train data. Default: 'train'.\n",
      "     |          validation: The prefix of the validation data. Default: 'val'.\n",
      "     |          test: The prefix of the test data. Default: 'test'.\n",
      "     |          Remaining keyword arguments: Passed to the splits method of\n",
      "     |              Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = ''\n",
      "     |  \n",
      "     |  name = 'multi30k'\n",
      "     |  \n",
      "     |  urls = ['http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar....\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TranslationDataset:\n",
      "     |  \n",
      "     |  __init__(self, path, exts, fields, **kwargs)\n",
      "     |      Create a TranslationDataset given paths and fields.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path: Common prefix of paths to the data files for both languages.\n",
      "     |          exts: A tuple containing the extension to path for each language.\n",
      "     |          fields: A tuple containing the fields that will be used for data\n",
      "     |              in each language.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of\n",
      "     |              data.Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from TranslationDataset:\n",
      "     |  \n",
      "     |  sort_key(ex)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MultiNLI(NLIDataset)\n",
      "     |  MultiNLI(path, format, fields, skip_header=False, csv_reader_params={}, **kwargs)\n",
      "     |  \n",
      "     |  Defines a Dataset of columns stored in CSV, TSV, or JSON format.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiNLI\n",
      "     |      NLIDataset\n",
      "     |      torchtext.data.dataset.TabularDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  splits(text_field, label_field, parse_field=None, genre_field=None, root='.data', train='multinli_1.0_train.jsonl', validation='multinli_1.0_dev_matched.jsonl', test='multinli_1.0_dev_mismatched.jsonl') from builtins.type\n",
      "     |      Create dataset objects for splits of the SNLI dataset.\n",
      "     |      \n",
      "     |      This is the most flexible way to use the dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          text_field: The field that will be used for premise and hypothesis\n",
      "     |              data.\n",
      "     |          label_field: The field that will be used for label data.\n",
      "     |          parse_field: The field that will be used for shift-reduce parser\n",
      "     |              transitions, or None to not include them.\n",
      "     |          extra_fields: A dict[json_key: Tuple(field_name, Field)]\n",
      "     |          root: The root directory that the dataset's zip archive will be\n",
      "     |              expanded into.\n",
      "     |          train: The filename of the train data. Default: 'train.jsonl'.\n",
      "     |          validation: The filename of the validation data, or None to not\n",
      "     |              load the validation set. Default: 'dev.jsonl'.\n",
      "     |          test: The filename of the test data, or None to not load the test\n",
      "     |              set. Default: 'test.jsonl'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = 'multinli_1.0'\n",
      "     |  \n",
      "     |  name = 'multinli'\n",
      "     |  \n",
      "     |  urls = ['http://www.nyu.edu/projects/bowman/multinli/multinli_1.0.zip'...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from NLIDataset:\n",
      "     |  \n",
      "     |  iters(batch_size=32, device=0, root='.data', vectors=None, trees=False, **kwargs) from builtins.type\n",
      "     |      Create iterator objects for splits of the SNLI dataset.\n",
      "     |      \n",
      "     |      This is the simplest way to use the dataset, and assumes common\n",
      "     |      defaults for field, vocabulary, and iterator parameters.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          batch_size: Batch size.\n",
      "     |          device: Device to create batches on. Use -1 for CPU and None for\n",
      "     |              the currently active GPU device.\n",
      "     |          root: The root directory that the dataset's zip archive will be\n",
      "     |              expanded into; therefore the directory in whose wikitext-2\n",
      "     |              subdirectory the data files will be stored.\n",
      "     |          vectors: one of the available pretrained vectors or a list with each\n",
      "     |              element one of the available pretrained vectors (see Vocab.load_vectors)\n",
      "     |          trees: Whether to include shift-reduce parser transitions.\n",
      "     |              Default: False.\n",
      "     |          Remaining keyword arguments: Passed to the splits method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from NLIDataset:\n",
      "     |  \n",
      "     |  sort_key(ex)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.TabularDataset:\n",
      "     |  \n",
      "     |  __init__(self, path, format, fields, skip_header=False, csv_reader_params={}, **kwargs)\n",
      "     |      Create a TabularDataset given a path, file format, and field list.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path (str): Path to the data file.\n",
      "     |          format (str): The format of the data file. One of \"CSV\", \"TSV\", or\n",
      "     |              \"JSON\" (case-insensitive).\n",
      "     |          fields (list(tuple(str, Field)) or dict[str: tuple(str, Field)]:\n",
      "     |              If using a list, the format must be CSV or TSV, and the values of the list\n",
      "     |              should be tuples of (name, field).\n",
      "     |              The fields should be in the same order as the columns in the CSV or TSV\n",
      "     |              file, while tuples of (name, None) represent columns that will be ignored.\n",
      "     |      \n",
      "     |              If using a dict, the keys should be a subset of the JSON keys or CSV/TSV\n",
      "     |              columns, and the values should be tuples of (name, field).\n",
      "     |              Keys not present in the input dictionary are ignored.\n",
      "     |              This allows the user to rename columns from their JSON/CSV/TSV key names\n",
      "     |              and also enables selecting a subset of columns to load.\n",
      "     |          skip_header (bool): Whether to skip the first line of the input file.\n",
      "     |          csv_reader_params(dict): Parameters to pass to the csv reader.\n",
      "     |              Only relevant when format is csv or tsv.\n",
      "     |              See\n",
      "     |              https://docs.python.org/3/library/csv.html#csv.reader\n",
      "     |              for more details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PennTreebank(LanguageModelingDataset)\n",
      "     |  PennTreebank(path, text_field, newline_eos=True, encoding='utf-8', **kwargs)\n",
      "     |  \n",
      "     |  The Penn Treebank dataset.\n",
      "     |  A relatively small dataset originally created for POS tagging.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Marcus, Mitchell P., Marcinkiewicz, Mary Ann & Santorini, Beatrice (1993).\n",
      "     |  Building a Large Annotated Corpus of English: The Penn Treebank\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PennTreebank\n",
      "     |      LanguageModelingDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  iters(batch_size=32, bptt_len=35, device=0, root='.data', vectors=None, **kwargs) from builtins.type\n",
      "     |      Create iterator objects for splits of the Penn Treebank dataset.\n",
      "     |      \n",
      "     |      This is the simplest way to use the dataset, and assumes common\n",
      "     |      defaults for field, vocabulary, and iterator parameters.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          batch_size: Batch size.\n",
      "     |          bptt_len: Length of sequences for backpropagation through time.\n",
      "     |          device: Device to create batches on. Use -1 for CPU and None for\n",
      "     |              the currently active GPU device.\n",
      "     |          root: The root directory where the data files will be stored.\n",
      "     |          wv_dir, wv_type, wv_dim: Passed to the Vocab constructor for the\n",
      "     |              text field. The word vectors are accessible as\n",
      "     |              train.dataset.fields['text'].vocab.vectors.\n",
      "     |          Remaining keyword arguments: Passed to the splits method.\n",
      "     |  \n",
      "     |  splits(text_field, root='.data', train='ptb.train.txt', validation='ptb.valid.txt', test='ptb.test.txt', **kwargs) from builtins.type\n",
      "     |      Create dataset objects for splits of the Penn Treebank dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          text_field: The field that will be used for text data.\n",
      "     |          root: The root directory where the data files will be stored.\n",
      "     |          train: The filename of the train data. Default: 'ptb.train.txt'.\n",
      "     |          validation: The filename of the validation data, or None to not\n",
      "     |              load the validation set. Default: 'ptb.valid.txt'.\n",
      "     |          test: The filename of the test data, or None to not load the test\n",
      "     |              set. Default: 'ptb.test.txt'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = ''\n",
      "     |  \n",
      "     |  name = 'penn-treebank'\n",
      "     |  \n",
      "     |  urls = ['https://raw.githubusercontent.com/wojzaremba/lstm/master/data...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LanguageModelingDataset:\n",
      "     |  \n",
      "     |  __init__(self, path, text_field, newline_eos=True, encoding='utf-8', **kwargs)\n",
      "     |      Create a LanguageModelingDataset given a path and a field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path: Path to the data file.\n",
      "     |          text_field: The field that will be used for text data.\n",
      "     |          newline_eos: Whether to add an <eos> token for every newline in the\n",
      "     |              data file. Default: True.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of\n",
      "     |              data.Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  sort_key = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SNLI(NLIDataset)\n",
      "     |  SNLI(path, format, fields, skip_header=False, csv_reader_params={}, **kwargs)\n",
      "     |  \n",
      "     |  Defines a Dataset of columns stored in CSV, TSV, or JSON format.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SNLI\n",
      "     |      NLIDataset\n",
      "     |      torchtext.data.dataset.TabularDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  splits(text_field, label_field, parse_field=None, root='.data', train='snli_1.0_train.jsonl', validation='snli_1.0_dev.jsonl', test='snli_1.0_test.jsonl') from builtins.type\n",
      "     |      Create dataset objects for splits of the SNLI dataset.\n",
      "     |      \n",
      "     |      This is the most flexible way to use the dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          text_field: The field that will be used for premise and hypothesis\n",
      "     |              data.\n",
      "     |          label_field: The field that will be used for label data.\n",
      "     |          parse_field: The field that will be used for shift-reduce parser\n",
      "     |              transitions, or None to not include them.\n",
      "     |          extra_fields: A dict[json_key: Tuple(field_name, Field)]\n",
      "     |          root: The root directory that the dataset's zip archive will be\n",
      "     |              expanded into.\n",
      "     |          train: The filename of the train data. Default: 'train.jsonl'.\n",
      "     |          validation: The filename of the validation data, or None to not\n",
      "     |              load the validation set. Default: 'dev.jsonl'.\n",
      "     |          test: The filename of the test data, or None to not load the test\n",
      "     |              set. Default: 'test.jsonl'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = 'snli_1.0'\n",
      "     |  \n",
      "     |  name = 'snli'\n",
      "     |  \n",
      "     |  urls = ['http://nlp.stanford.edu/projects/snli/snli_1.0.zip']\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from NLIDataset:\n",
      "     |  \n",
      "     |  iters(batch_size=32, device=0, root='.data', vectors=None, trees=False, **kwargs) from builtins.type\n",
      "     |      Create iterator objects for splits of the SNLI dataset.\n",
      "     |      \n",
      "     |      This is the simplest way to use the dataset, and assumes common\n",
      "     |      defaults for field, vocabulary, and iterator parameters.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          batch_size: Batch size.\n",
      "     |          device: Device to create batches on. Use -1 for CPU and None for\n",
      "     |              the currently active GPU device.\n",
      "     |          root: The root directory that the dataset's zip archive will be\n",
      "     |              expanded into; therefore the directory in whose wikitext-2\n",
      "     |              subdirectory the data files will be stored.\n",
      "     |          vectors: one of the available pretrained vectors or a list with each\n",
      "     |              element one of the available pretrained vectors (see Vocab.load_vectors)\n",
      "     |          trees: Whether to include shift-reduce parser transitions.\n",
      "     |              Default: False.\n",
      "     |          Remaining keyword arguments: Passed to the splits method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from NLIDataset:\n",
      "     |  \n",
      "     |  sort_key(ex)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.TabularDataset:\n",
      "     |  \n",
      "     |  __init__(self, path, format, fields, skip_header=False, csv_reader_params={}, **kwargs)\n",
      "     |      Create a TabularDataset given a path, file format, and field list.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path (str): Path to the data file.\n",
      "     |          format (str): The format of the data file. One of \"CSV\", \"TSV\", or\n",
      "     |              \"JSON\" (case-insensitive).\n",
      "     |          fields (list(tuple(str, Field)) or dict[str: tuple(str, Field)]:\n",
      "     |              If using a list, the format must be CSV or TSV, and the values of the list\n",
      "     |              should be tuples of (name, field).\n",
      "     |              The fields should be in the same order as the columns in the CSV or TSV\n",
      "     |              file, while tuples of (name, None) represent columns that will be ignored.\n",
      "     |      \n",
      "     |              If using a dict, the keys should be a subset of the JSON keys or CSV/TSV\n",
      "     |              columns, and the values should be tuples of (name, field).\n",
      "     |              Keys not present in the input dictionary are ignored.\n",
      "     |              This allows the user to rename columns from their JSON/CSV/TSV key names\n",
      "     |              and also enables selecting a subset of columns to load.\n",
      "     |          skip_header (bool): Whether to skip the first line of the input file.\n",
      "     |          csv_reader_params(dict): Parameters to pass to the csv reader.\n",
      "     |              Only relevant when format is csv or tsv.\n",
      "     |              See\n",
      "     |              https://docs.python.org/3/library/csv.html#csv.reader\n",
      "     |              for more details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SST(torchtext.data.dataset.Dataset)\n",
      "     |  SST(path, text_field, label_field, subtrees=False, fine_grained=False, **kwargs)\n",
      "     |  \n",
      "     |  Defines a dataset composed of Examples along with its Fields.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      sort_key (callable): A key to use for sorting dataset examples for batching\n",
      "     |          together examples with similar lengths to minimize padding.\n",
      "     |      examples (list(Example)): The examples in this dataset.\n",
      "     |      fields (dict[str, Field]): Contains the name of each column or field, together\n",
      "     |          with the corresponding Field object. Two fields with the same Field object\n",
      "     |          will have a shared vocabulary.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SST\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, path, text_field, label_field, subtrees=False, fine_grained=False, **kwargs)\n",
      "     |      Create an SST dataset instance given a path and fields.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path: Path to the data file\n",
      "     |          text_field: The field that will be used for text data.\n",
      "     |          label_field: The field that will be used for label data.\n",
      "     |          subtrees: Whether to include sentiment-tagged subphrases\n",
      "     |              in addition to complete examples. Default: False.\n",
      "     |          fine_grained: Whether to use 5-class instead of 3-class\n",
      "     |              labeling. Default: False.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of\n",
      "     |              data.Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  iters(batch_size=32, device=0, root='.data', vectors=None, **kwargs) from builtins.type\n",
      "     |      Create iterator objects for splits of the SST dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          batch_size: Batch_size\n",
      "     |          device: Device to create batches on. Use - 1 for CPU and None for\n",
      "     |              the currently active GPU device.\n",
      "     |          root: The root directory that the dataset's zip archive will be\n",
      "     |              expanded into; therefore the directory in whose trees\n",
      "     |              subdirectory the data files will be stored.\n",
      "     |          vectors: one of the available pretrained vectors or a list with each\n",
      "     |              element one of the available pretrained vectors (see Vocab.load_vectors)\n",
      "     |          Remaining keyword arguments: Passed to the splits method.\n",
      "     |  \n",
      "     |  splits(text_field, label_field, root='.data', train='train.txt', validation='dev.txt', test='test.txt', train_subtrees=False, **kwargs) from builtins.type\n",
      "     |      Create dataset objects for splits of the SST dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          text_field: The field that will be used for the sentence.\n",
      "     |          label_field: The field that will be used for label data.\n",
      "     |          root: The root directory that the dataset's zip archive will be\n",
      "     |              expanded into; therefore the directory in whose trees\n",
      "     |              subdirectory the data files will be stored.\n",
      "     |          train: The filename of the train data. Default: 'train.txt'.\n",
      "     |          validation: The filename of the validation data, or None to not\n",
      "     |              load the validation set. Default: 'dev.txt'.\n",
      "     |          test: The filename of the test data, or None to not load the test\n",
      "     |              set. Default: 'test.txt'.\n",
      "     |          train_subtrees: Whether to use all subtrees in the training set.\n",
      "     |              Default: False.\n",
      "     |          Remaining keyword arguments: Passed to the splits method of\n",
      "     |              Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  sort_key(ex)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = 'trees'\n",
      "     |  \n",
      "     |  name = 'sst'\n",
      "     |  \n",
      "     |  urls = ['http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip']\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SequenceTaggingDataset(torchtext.data.dataset.Dataset)\n",
      "     |  SequenceTaggingDataset(path, fields, encoding='utf-8', separator='\\t', **kwargs)\n",
      "     |  \n",
      "     |  Defines a dataset for sequence tagging. Examples in this dataset\n",
      "     |  contain paired lists -- paired list of words and tags.\n",
      "     |  \n",
      "     |  For example, in the case of part-of-speech tagging, an example is of the\n",
      "     |  form\n",
      "     |  [I, love, PyTorch, .] paired with [PRON, VERB, PROPN, PUNCT]\n",
      "     |  \n",
      "     |  See torchtext/test/sequence_tagging.py on how to use this class.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SequenceTaggingDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, path, fields, encoding='utf-8', separator='\\t', **kwargs)\n",
      "     |      Create a dataset from a list of Examples and Fields.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          examples: List of Examples.\n",
      "     |          fields (List(tuple(str, Field))): The Fields to use in this tuple. The\n",
      "     |              string is a field name, and the Field is the associated field.\n",
      "     |          filter_pred (callable or None): Use only examples for which\n",
      "     |              filter_pred(example) is True, or use all examples if None.\n",
      "     |              Default is None.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  sort_key(example)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  splits(path=None, root='.data', train=None, validation=None, test=None, **kwargs) from builtins.type\n",
      "     |      Create Dataset objects for multiple splits of a dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path (str): Common prefix of the splits' file paths, or None to use\n",
      "     |              the result of cls.download(root).\n",
      "     |          root (str): Root dataset storage directory. Default is '.data'.\n",
      "     |          train (str): Suffix to add to path for the train set, or None for no\n",
      "     |              train set. Default is None.\n",
      "     |          validation (str): Suffix to add to path for the validation set, or None\n",
      "     |              for no validation set. Default is None.\n",
      "     |          test (str): Suffix to add to path for the test set, or None for no test\n",
      "     |              set. Default is None.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of the\n",
      "     |              Dataset (sub)class being used.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TREC(torchtext.data.dataset.Dataset)\n",
      "     |  TREC(path, text_field, label_field, fine_grained=False, **kwargs)\n",
      "     |  \n",
      "     |  Defines a dataset composed of Examples along with its Fields.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      sort_key (callable): A key to use for sorting dataset examples for batching\n",
      "     |          together examples with similar lengths to minimize padding.\n",
      "     |      examples (list(Example)): The examples in this dataset.\n",
      "     |      fields (dict[str, Field]): Contains the name of each column or field, together\n",
      "     |          with the corresponding Field object. Two fields with the same Field object\n",
      "     |          will have a shared vocabulary.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TREC\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, path, text_field, label_field, fine_grained=False, **kwargs)\n",
      "     |      Create an TREC dataset instance given a path and fields.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path: Path to the data file.\n",
      "     |          text_field: The field that will be used for text data.\n",
      "     |          label_field: The field that will be used for label data.\n",
      "     |          fine_grained: Whether to use the fine-grained (50-class) version of TREC\n",
      "     |              or the coarse grained (6-class) version.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of\n",
      "     |              data.Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  iters(batch_size=32, device=0, root='.data', vectors=None, **kwargs) from builtins.type\n",
      "     |      Create iterator objects for splits of the TREC dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          batch_size: Batch_size\n",
      "     |          device: Device to create batches on. Use - 1 for CPU and None for\n",
      "     |              the currently active GPU device.\n",
      "     |          root: The root directory that contains the trec dataset subdirectory\n",
      "     |          vectors: one of the available pretrained vectors or a list with each\n",
      "     |              element one of the available pretrained vectors (see Vocab.load_vectors)\n",
      "     |          Remaining keyword arguments: Passed to the splits method.\n",
      "     |  \n",
      "     |  splits(text_field, label_field, root='.data', train='train_5500.label', test='TREC_10.label', **kwargs) from builtins.type\n",
      "     |      Create dataset objects for splits of the TREC dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          text_field: The field that will be used for the sentence.\n",
      "     |          label_field: The field that will be used for label data.\n",
      "     |          root: Root dataset storage directory. Default is '.data'.\n",
      "     |          train: The filename of the train data. Default: 'train_5500.label'.\n",
      "     |          test: The filename of the test data, or None to not load the test\n",
      "     |              set. Default: 'TREC_10.label'.\n",
      "     |          Remaining keyword arguments: Passed to the splits method of\n",
      "     |              Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  sort_key(ex)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = ''\n",
      "     |  \n",
      "     |  name = 'trec'\n",
      "     |  \n",
      "     |  urls = ['http://cogcomp.org/Data/QA/QC/train_5500.label', 'http://cogc...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TextClassificationDataset(torch.utils.data.dataset.Dataset)\n",
      "     |  TextClassificationDataset(vocab, data, labels)\n",
      "     |  \n",
      "     |  Defines an abstract text classification datasets.\n",
      "     |  Currently, we only support the following datasets:\n",
      "     |  \n",
      "     |        - AG_NEWS\n",
      "     |        - SogouNews\n",
      "     |        - DBpedia\n",
      "     |        - YelpReviewPolarity\n",
      "     |        - YelpReviewFull\n",
      "     |        - YahooAnswers\n",
      "     |        - AmazonReviewPolarity\n",
      "     |        - AmazonReviewFull\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TextClassificationDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __init__(self, vocab, data, labels)\n",
      "     |      Initiate text-classification dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          vocab: Vocabulary object used for dataset.\n",
      "     |          data: a list of label/tokens tuple. tokens are a tensor after\n",
      "     |              numericalizing the string tokens. label is an integer.\n",
      "     |              [(label1, tokens1), (label2, tokens2), (label2, tokens3)]\n",
      "     |          label: a set of the labels.\n",
      "     |              {label1, label2}\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |          See the examples in examples/text_classification/\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  get_labels(self)\n",
      "     |  \n",
      "     |  get_vocab(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TranslationDataset(torchtext.data.dataset.Dataset)\n",
      "     |  TranslationDataset(path, exts, fields, **kwargs)\n",
      "     |  \n",
      "     |  Defines a dataset for machine translation.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TranslationDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, path, exts, fields, **kwargs)\n",
      "     |      Create a TranslationDataset given paths and fields.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path: Common prefix of paths to the data files for both languages.\n",
      "     |          exts: A tuple containing the extension to path for each language.\n",
      "     |          fields: A tuple containing the fields that will be used for data\n",
      "     |              in each language.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of\n",
      "     |              data.Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  splits(exts, fields, path=None, root='.data', train='train', validation='val', test='test', **kwargs) from builtins.type\n",
      "     |      Create dataset objects for splits of a TranslationDataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          exts: A tuple containing the extension to path for each language.\n",
      "     |          fields: A tuple containing the fields that will be used for data\n",
      "     |              in each language.\n",
      "     |          path (str): Common prefix of the splits' file paths, or None to use\n",
      "     |              the result of cls.download(root).\n",
      "     |          root: Root dataset storage directory. Default is '.data'.\n",
      "     |          train: The prefix of the train data. Default: 'train'.\n",
      "     |          validation: The prefix of the validation data. Default: 'val'.\n",
      "     |          test: The prefix of the test data. Default: 'test'.\n",
      "     |          Remaining keyword arguments: Passed to the splits method of\n",
      "     |              Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  sort_key(ex)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class UDPOS(SequenceTaggingDataset)\n",
      "     |  UDPOS(path, fields, encoding='utf-8', separator='\\t', **kwargs)\n",
      "     |  \n",
      "     |  Defines a dataset for sequence tagging. Examples in this dataset\n",
      "     |  contain paired lists -- paired list of words and tags.\n",
      "     |  \n",
      "     |  For example, in the case of part-of-speech tagging, an example is of the\n",
      "     |  form\n",
      "     |  [I, love, PyTorch, .] paired with [PRON, VERB, PROPN, PUNCT]\n",
      "     |  \n",
      "     |  See torchtext/test/sequence_tagging.py on how to use this class.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      UDPOS\n",
      "     |      SequenceTaggingDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  splits(fields, root='.data', train='en-ud-tag.v2.train.txt', validation='en-ud-tag.v2.dev.txt', test='en-ud-tag.v2.test.txt', **kwargs) from builtins.type\n",
      "     |      Downloads and loads the Universal Dependencies Version 2 POS Tagged\n",
      "     |      data.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = 'en-ud-v2'\n",
      "     |  \n",
      "     |  name = 'udpos'\n",
      "     |  \n",
      "     |  urls = ['https://bitbucket.org/sivareddyg/public/downloads/en-ud-v2.zi...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SequenceTaggingDataset:\n",
      "     |  \n",
      "     |  __init__(self, path, fields, encoding='utf-8', separator='\\t', **kwargs)\n",
      "     |      Create a dataset from a list of Examples and Fields.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          examples: List of Examples.\n",
      "     |          fields (List(tuple(str, Field))): The Fields to use in this tuple. The\n",
      "     |              string is a field name, and the Field is the associated field.\n",
      "     |          filter_pred (callable or None): Use only examples for which\n",
      "     |              filter_pred(example) is True, or use all examples if None.\n",
      "     |              Default is None.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from SequenceTaggingDataset:\n",
      "     |  \n",
      "     |  sort_key(example)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class WMT14(TranslationDataset)\n",
      "     |  WMT14(path, exts, fields, **kwargs)\n",
      "     |  \n",
      "     |  The WMT 2014 English-German dataset, as preprocessed by Google Brain.\n",
      "     |  \n",
      "     |  Though this download contains test sets from 2015 and 2016, the train set\n",
      "     |  differs slightly from WMT 2015 and 2016 and significantly from WMT 2017.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WMT14\n",
      "     |      TranslationDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  splits(exts, fields, root='.data', train='train.tok.clean.bpe.32000', validation='newstest2013.tok.bpe.32000', test='newstest2014.tok.bpe.32000', **kwargs) from builtins.type\n",
      "     |      Create dataset objects for splits of the WMT 2014 dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          exts: A tuple containing the extensions for each language. Must be\n",
      "     |              either ('.en', '.de') or the reverse.\n",
      "     |          fields: A tuple containing the fields that will be used for data\n",
      "     |              in each language.\n",
      "     |          root: Root dataset storage directory. Default is '.data'.\n",
      "     |          train: The prefix of the train data. Default:\n",
      "     |              'train.tok.clean.bpe.32000'.\n",
      "     |          validation: The prefix of the validation data. Default:\n",
      "     |              'newstest2013.tok.bpe.32000'.\n",
      "     |          test: The prefix of the test data. Default:\n",
      "     |              'newstest2014.tok.bpe.32000'.\n",
      "     |          Remaining keyword arguments: Passed to the splits method of\n",
      "     |              Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = ''\n",
      "     |  \n",
      "     |  name = 'wmt14'\n",
      "     |  \n",
      "     |  urls = [('https://drive.google.com/uc?export=download&id=0B_bZck-ksdkp...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TranslationDataset:\n",
      "     |  \n",
      "     |  __init__(self, path, exts, fields, **kwargs)\n",
      "     |      Create a TranslationDataset given paths and fields.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path: Common prefix of paths to the data files for both languages.\n",
      "     |          exts: A tuple containing the extension to path for each language.\n",
      "     |          fields: A tuple containing the fields that will be used for data\n",
      "     |              in each language.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of\n",
      "     |              data.Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from TranslationDataset:\n",
      "     |  \n",
      "     |  sort_key(ex)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class WikiText103(LanguageModelingDataset)\n",
      "     |  WikiText103(path, text_field, newline_eos=True, encoding='utf-8', **kwargs)\n",
      "     |  \n",
      "     |  Defines a dataset for language modeling.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WikiText103\n",
      "     |      LanguageModelingDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  iters(batch_size=32, bptt_len=35, device=0, root='.data', vectors=None, **kwargs) from builtins.type\n",
      "     |      Create iterator objects for splits of the WikiText-103 dataset.\n",
      "     |      \n",
      "     |      This is the simplest way to use the dataset, and assumes common\n",
      "     |      defaults for field, vocabulary, and iterator parameters.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          batch_size: Batch size.\n",
      "     |          bptt_len: Length of sequences for backpropagation through time.\n",
      "     |          device: Device to create batches on. Use -1 for CPU and None for\n",
      "     |              the currently active GPU device.\n",
      "     |          root: The root directory that the dataset's zip archive will be\n",
      "     |              expanded into; therefore the directory in whose wikitext-2\n",
      "     |              subdirectory the data files will be stored.\n",
      "     |          wv_dir, wv_type, wv_dim: Passed to the Vocab constructor for the\n",
      "     |              text field. The word vectors are accessible as\n",
      "     |              train.dataset.fields['text'].vocab.vectors.\n",
      "     |          Remaining keyword arguments: Passed to the splits method.\n",
      "     |  \n",
      "     |  splits(text_field, root='.data', train='wiki.train.tokens', validation='wiki.valid.tokens', test='wiki.test.tokens', **kwargs) from builtins.type\n",
      "     |      Create dataset objects for splits of the WikiText-103 dataset.\n",
      "     |      \n",
      "     |      This is the most flexible way to use the dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          text_field: The field that will be used for text data.\n",
      "     |          root: The root directory that the dataset's zip archive will be\n",
      "     |              expanded into; therefore the directory in whose wikitext-103\n",
      "     |              subdirectory the data files will be stored.\n",
      "     |          train: The filename of the train data. Default: 'wiki.train.tokens'.\n",
      "     |          validation: The filename of the validation data, or None to not\n",
      "     |              load the validation set. Default: 'wiki.valid.tokens'.\n",
      "     |          test: The filename of the test data, or None to not load the test\n",
      "     |              set. Default: 'wiki.test.tokens'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = 'wikitext-103'\n",
      "     |  \n",
      "     |  name = 'wikitext-103'\n",
      "     |  \n",
      "     |  urls = ['https://s3.amazonaws.com/research.metamind.io/wikitext/wikite...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LanguageModelingDataset:\n",
      "     |  \n",
      "     |  __init__(self, path, text_field, newline_eos=True, encoding='utf-8', **kwargs)\n",
      "     |      Create a LanguageModelingDataset given a path and a field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path: Path to the data file.\n",
      "     |          text_field: The field that will be used for text data.\n",
      "     |          newline_eos: Whether to add an <eos> token for every newline in the\n",
      "     |              data file. Default: True.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of\n",
      "     |              data.Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  sort_key = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class WikiText2(LanguageModelingDataset)\n",
      "     |  WikiText2(path, text_field, newline_eos=True, encoding='utf-8', **kwargs)\n",
      "     |  \n",
      "     |  Defines a dataset for language modeling.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WikiText2\n",
      "     |      LanguageModelingDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  iters(batch_size=32, bptt_len=35, device=0, root='.data', vectors=None, **kwargs) from builtins.type\n",
      "     |      Create iterator objects for splits of the WikiText-2 dataset.\n",
      "     |      \n",
      "     |      This is the simplest way to use the dataset, and assumes common\n",
      "     |      defaults for field, vocabulary, and iterator parameters.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          batch_size: Batch size.\n",
      "     |          bptt_len: Length of sequences for backpropagation through time.\n",
      "     |          device: Device to create batches on. Use -1 for CPU and None for\n",
      "     |              the currently active GPU device.\n",
      "     |          root: The root directory that the dataset's zip archive will be\n",
      "     |              expanded into; therefore the directory in whose wikitext-2\n",
      "     |              subdirectory the data files will be stored.\n",
      "     |          wv_dir, wv_type, wv_dim: Passed to the Vocab constructor for the\n",
      "     |              text field. The word vectors are accessible as\n",
      "     |              train.dataset.fields['text'].vocab.vectors.\n",
      "     |          Remaining keyword arguments: Passed to the splits method.\n",
      "     |  \n",
      "     |  splits(text_field, root='.data', train='wiki.train.tokens', validation='wiki.valid.tokens', test='wiki.test.tokens', **kwargs) from builtins.type\n",
      "     |      Create dataset objects for splits of the WikiText-2 dataset.\n",
      "     |      \n",
      "     |      This is the most flexible way to use the dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          text_field: The field that will be used for text data.\n",
      "     |          root: The root directory that the dataset's zip archive will be\n",
      "     |              expanded into; therefore the directory in whose wikitext-2\n",
      "     |              subdirectory the data files will be stored.\n",
      "     |          train: The filename of the train data. Default: 'wiki.train.tokens'.\n",
      "     |          validation: The filename of the validation data, or None to not\n",
      "     |              load the validation set. Default: 'wiki.valid.tokens'.\n",
      "     |          test: The filename of the test data, or None to not load the test\n",
      "     |              set. Default: 'wiki.test.tokens'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = 'wikitext-2'\n",
      "     |  \n",
      "     |  name = 'wikitext-2'\n",
      "     |  \n",
      "     |  urls = ['https://s3.amazonaws.com/research.metamind.io/wikitext/wikite...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LanguageModelingDataset:\n",
      "     |  \n",
      "     |  __init__(self, path, text_field, newline_eos=True, encoding='utf-8', **kwargs)\n",
      "     |      Create a LanguageModelingDataset given a path and a field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path: Path to the data file.\n",
      "     |          text_field: The field that will be used for text data.\n",
      "     |          newline_eos: Whether to add an <eos> token for every newline in the\n",
      "     |              data file. Default: True.\n",
      "     |          Remaining keyword arguments: Passed to the constructor of\n",
      "     |              data.Dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  sort_key = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XNLI(NLIDataset)\n",
      "     |  XNLI(path, format, fields, skip_header=False, csv_reader_params={}, **kwargs)\n",
      "     |  \n",
      "     |  Defines a Dataset of columns stored in CSV, TSV, or JSON format.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XNLI\n",
      "     |      NLIDataset\n",
      "     |      torchtext.data.dataset.TabularDataset\n",
      "     |      torchtext.data.dataset.Dataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  iters(*args, **kwargs) from builtins.type\n",
      "     |      Create iterator objects for splits of the SNLI dataset.\n",
      "     |      \n",
      "     |      This is the simplest way to use the dataset, and assumes common\n",
      "     |      defaults for field, vocabulary, and iterator parameters.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          batch_size: Batch size.\n",
      "     |          device: Device to create batches on. Use -1 for CPU and None for\n",
      "     |              the currently active GPU device.\n",
      "     |          root: The root directory that the dataset's zip archive will be\n",
      "     |              expanded into; therefore the directory in whose wikitext-2\n",
      "     |              subdirectory the data files will be stored.\n",
      "     |          vectors: one of the available pretrained vectors or a list with each\n",
      "     |              element one of the available pretrained vectors (see Vocab.load_vectors)\n",
      "     |          trees: Whether to include shift-reduce parser transitions.\n",
      "     |              Default: False.\n",
      "     |          Remaining keyword arguments: Passed to the splits method.\n",
      "     |  \n",
      "     |  splits(text_field, label_field, genre_field=None, language_field=None, root='.data', validation='xnli.dev.jsonl', test='xnli.test.jsonl') from builtins.type\n",
      "     |      Create dataset objects for splits of the SNLI dataset.\n",
      "     |      \n",
      "     |      This is the most flexible way to use the dataset.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          text_field: The field that will be used for premise and hypothesis\n",
      "     |              data.\n",
      "     |          label_field: The field that will be used for label data.\n",
      "     |          parse_field: The field that will be used for shift-reduce parser\n",
      "     |              transitions, or None to not include them.\n",
      "     |          extra_fields: A dict[json_key: Tuple(field_name, Field)]\n",
      "     |          root: The root directory that the dataset's zip archive will be\n",
      "     |              expanded into.\n",
      "     |          train: The filename of the train data. Default: 'train.jsonl'.\n",
      "     |          validation: The filename of the validation data, or None to not\n",
      "     |              load the validation set. Default: 'dev.jsonl'.\n",
      "     |          test: The filename of the test data, or None to not load the test\n",
      "     |              set. Default: 'test.jsonl'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  dirname = 'XNLI-1.0'\n",
      "     |  \n",
      "     |  name = 'xnli'\n",
      "     |  \n",
      "     |  urls = ['http://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip']\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from NLIDataset:\n",
      "     |  \n",
      "     |  sort_key(ex)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.TabularDataset:\n",
      "     |  \n",
      "     |  __init__(self, path, format, fields, skip_header=False, csv_reader_params={}, **kwargs)\n",
      "     |      Create a TabularDataset given a path, file format, and field list.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          path (str): Path to the data file.\n",
      "     |          format (str): The format of the data file. One of \"CSV\", \"TSV\", or\n",
      "     |              \"JSON\" (case-insensitive).\n",
      "     |          fields (list(tuple(str, Field)) or dict[str: tuple(str, Field)]:\n",
      "     |              If using a list, the format must be CSV or TSV, and the values of the list\n",
      "     |              should be tuples of (name, field).\n",
      "     |              The fields should be in the same order as the columns in the CSV or TSV\n",
      "     |              file, while tuples of (name, None) represent columns that will be ignored.\n",
      "     |      \n",
      "     |              If using a dict, the keys should be a subset of the JSON keys or CSV/TSV\n",
      "     |              columns, and the values should be tuples of (name, field).\n",
      "     |              Keys not present in the input dictionary are ignored.\n",
      "     |              This allows the user to rename columns from their JSON/CSV/TSV key names\n",
      "     |              and also enables selecting a subset of columns to load.\n",
      "     |          skip_header (bool): Whether to skip the first line of the input file.\n",
      "     |          csv_reader_params(dict): Parameters to pass to the csv reader.\n",
      "     |              Only relevant when format is csv or tsv.\n",
      "     |              See\n",
      "     |              https://docs.python.org/3/library/csv.html#csv.reader\n",
      "     |              for more details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __getattr__(self, attr)\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter_examples(self, field_names)\n",
      "     |      Remove unknown words from dataset examples with respect to given field.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          field_names (list(str)): Within example only the parts with field names in\n",
      "     |              field_names will have their unknown words deleted.\n",
      "     |  \n",
      "     |  split(self, split_ratio=0.7, stratified=False, strata_field='label', random_state=None)\n",
      "     |      Create train-test(-valid?) splits from the instance's examples.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          split_ratio (float or List of floats): a number [0, 1] denoting the amount\n",
      "     |              of data to be used for the training split (rest is used for test),\n",
      "     |              or a list of numbers denoting the relative sizes of train, test and valid\n",
      "     |              splits respectively. If the relative size for valid is missing, only the\n",
      "     |              train-test split is returned. Default is 0.7 (for the train set).\n",
      "     |          stratified (bool): whether the sampling should be stratified.\n",
      "     |              Default is False.\n",
      "     |          strata_field (str): name of the examples Field stratified over.\n",
      "     |              Default is 'label' for the conventional label field.\n",
      "     |          random_state (tuple): the random seed used for shuffling.\n",
      "     |              A return value of `random.getstate()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tuple[Dataset]: Datasets for train, validation, and\n",
      "     |          test splits in that order, if the splits are provided.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torchtext.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  download(root, check=None) from builtins.type\n",
      "     |      Download and unzip an online archive (.zip, .gz, or .tgz).\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          root (str): Folder to download data to.\n",
      "     |          check (str or None): Folder whose existence indicates\n",
      "     |              that the dataset has already been downloaded, or\n",
      "     |              None to check the existence of root/{cls.name}.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Path to extracted dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    AG_NEWS(*args, **kwargs)\n",
      "        Defines AG_NEWS datasets.\n",
      "            The labels includes:\n",
      "                - 1 : World\n",
      "                - 2 : Sports\n",
      "                - 3 : Business\n",
      "                - 4 : Sci/Tech\n",
      "        \n",
      "        Create supervised learning dataset: AG_NEWS\n",
      "        \n",
      "        Separately returns the training and test dataset\n",
      "        \n",
      "        Arguments:\n",
      "            root: Directory where the datasets are saved. Default: \".data\"\n",
      "            ngrams: a contiguous sequence of n items from s string text.\n",
      "                Default: 1\n",
      "            vocab: Vocabulary used for dataset. If None, it will generate a new\n",
      "                vocabulary based on the train data set.\n",
      "            include_unk: include unknown token in the data (Default: False)\n",
      "        \n",
      "        Examples:\n",
      "            >>> train_dataset, test_dataset = torchtext.datasets.AG_NEWS(ngrams=3)\n",
      "    \n",
      "    AmazonReviewFull(*args, **kwargs)\n",
      "        Defines AmazonReviewFull datasets.\n",
      "            The labels includes:\n",
      "                1 - 5 : rating classes (5 is highly recommended)\n",
      "        \n",
      "        Create supervised learning dataset: AmazonReviewFull\n",
      "        \n",
      "        Separately returns the training and test dataset\n",
      "        \n",
      "        Arguments:\n",
      "            root: Directory where the dataset are saved. Default: \".data\"\n",
      "            ngrams: a contiguous sequence of n items from s string text.\n",
      "                Default: 1\n",
      "            vocab: Vocabulary used for dataset. If None, it will generate a new\n",
      "                vocabulary based on the train data set.\n",
      "            include_unk: include unknown token in the data (Default: False)\n",
      "        \n",
      "        Examples:\n",
      "            >>> train_dataset, test_dataset = torchtext.datasets.AmazonReviewFull(ngrams=3)\n",
      "    \n",
      "    AmazonReviewPolarity(*args, **kwargs)\n",
      "        Defines AmazonReviewPolarity datasets.\n",
      "            The labels includes:\n",
      "                - 1 : Negative polarity\n",
      "                - 2 : Positive polarity\n",
      "        \n",
      "        Create supervised learning dataset: AmazonReviewPolarity\n",
      "        \n",
      "        Separately returns the training and test dataset\n",
      "        \n",
      "        Arguments:\n",
      "            root: Directory where the datasets are saved. Default: \".data\"\n",
      "            ngrams: a contiguous sequence of n items from s string text.\n",
      "                Default: 1\n",
      "            vocab: Vocabulary used for dataset. If None, it will generate a new\n",
      "                vocabulary based on the train data set.\n",
      "            include_unk: include unknown token in the data (Default: False)\n",
      "        \n",
      "        Examples:\n",
      "           >>> train_dataset, test_dataset = torchtext.datasets.AmazonReviewPolarity(ngrams=3)\n",
      "    \n",
      "    DBpedia(*args, **kwargs)\n",
      "        Defines DBpedia datasets.\n",
      "            The labels includes:\n",
      "                - 1 : Company\n",
      "                - 2 : EducationalInstitution\n",
      "                - 3 : Artist\n",
      "                - 4 : Athlete\n",
      "                - 5 : OfficeHolder\n",
      "                - 6 : MeanOfTransportation\n",
      "                - 7 : Building\n",
      "                - 8 : NaturalPlace\n",
      "                - 9 : Village\n",
      "                - 10 : Animal\n",
      "                - 11 : Plant\n",
      "                - 12 : Album\n",
      "                - 13 : Film\n",
      "                - 14 : WrittenWork\n",
      "        \n",
      "        Create supervised learning dataset: DBpedia\n",
      "        \n",
      "        Separately returns the training and test dataset\n",
      "        \n",
      "        Arguments:\n",
      "            root: Directory where the datasets are saved. Default: \".data\"\n",
      "            ngrams: a contiguous sequence of n items from s string text.\n",
      "                Default: 1\n",
      "            vocab: Vocabulary used for dataset. If None, it will generate a new\n",
      "                vocabulary based on the train data set.\n",
      "            include_unk: include unknown token in the data (Default: False)\n",
      "        \n",
      "        Examples:\n",
      "            >>> train_dataset, test_dataset = torchtext.datasets.DBpedia(ngrams=3)\n",
      "    \n",
      "    SogouNews(*args, **kwargs)\n",
      "        Defines SogouNews datasets.\n",
      "            The labels includes:\n",
      "                - 1 : Sports\n",
      "                - 2 : Finance\n",
      "                - 3 : Entertainment\n",
      "                - 4 : Automobile\n",
      "                - 5 : Technology\n",
      "        \n",
      "        Create supervised learning dataset: SogouNews\n",
      "        \n",
      "        Separately returns the training and test dataset\n",
      "        \n",
      "        Arguments:\n",
      "            root: Directory where the datasets are saved. Default: \".data\"\n",
      "            ngrams: a contiguous sequence of n items from s string text.\n",
      "                Default: 1\n",
      "            vocab: Vocabulary used for dataset. If None, it will generate a new\n",
      "                vocabulary based on the train data set.\n",
      "            include_unk: include unknown token in the data (Default: False)\n",
      "        \n",
      "        Examples:\n",
      "            >>> train_dataset, test_dataset = torchtext.datasets.SogouNews(ngrams=3)\n",
      "    \n",
      "    YahooAnswers(*args, **kwargs)\n",
      "        Defines YahooAnswers datasets.\n",
      "            The labels includes:\n",
      "                - 1 : Society & Culture\n",
      "                - 2 : Science & Mathematics\n",
      "                - 3 : Health\n",
      "                - 4 : Education & Reference\n",
      "                - 5 : Computers & Internet\n",
      "                - 6 : Sports\n",
      "                - 7 : Business & Finance\n",
      "                - 8 : Entertainment & Music\n",
      "                - 9 : Family & Relationships\n",
      "                - 10 : Politics & Government\n",
      "        \n",
      "        Create supervised learning dataset: YahooAnswers\n",
      "        \n",
      "        Separately returns the training and test dataset\n",
      "        \n",
      "        Arguments:\n",
      "            root: Directory where the datasets are saved. Default: \".data\"\n",
      "            ngrams: a contiguous sequence of n items from s string text.\n",
      "                Default: 1\n",
      "            vocab: Vocabulary used for dataset. If None, it will generate a new\n",
      "                vocabulary based on the train data set.\n",
      "            include_unk: include unknown token in the data (Default: False)\n",
      "        \n",
      "        Examples:\n",
      "            >>> train_dataset, test_dataset = torchtext.datasets.YahooAnswers(ngrams=3)\n",
      "    \n",
      "    YelpReviewFull(*args, **kwargs)\n",
      "        Defines YelpReviewFull datasets.\n",
      "            The labels includes:\n",
      "                1 - 5 : rating classes (5 is highly recommended).\n",
      "        \n",
      "        Create supervised learning dataset: YelpReviewFull\n",
      "        \n",
      "        Separately returns the training and test dataset\n",
      "        \n",
      "        Arguments:\n",
      "            root: Directory where the datasets are saved. Default: \".data\"\n",
      "            ngrams: a contiguous sequence of n items from s string text.\n",
      "                Default: 1\n",
      "            vocab: Vocabulary used for dataset. If None, it will generate a new\n",
      "                vocabulary based on the train data set.\n",
      "            include_unk: include unknown token in the data (Default: False)\n",
      "        \n",
      "        Examples:\n",
      "            >>> train_dataset, test_dataset = torchtext.datasets.YelpReviewFull(ngrams=3)\n",
      "    \n",
      "    YelpReviewPolarity(*args, **kwargs)\n",
      "        Defines YelpReviewPolarity datasets.\n",
      "            The labels includes:\n",
      "                - 1 : Negative polarity.\n",
      "                - 2 : Positive polarity.\n",
      "        \n",
      "        Create supervised learning dataset: YelpReviewPolarity\n",
      "        \n",
      "        Separately returns the training and test dataset\n",
      "        \n",
      "        Arguments:\n",
      "            root: Directory where the datasets are saved. Default: \".data\"\n",
      "            ngrams: a contiguous sequence of n items from s string text.\n",
      "                Default: 1\n",
      "            vocab: Vocabulary used for dataset. If None, it will generate a new\n",
      "                vocabulary based on the train data set.\n",
      "            include_unk: include unknown token in the data (Default: False)\n",
      "        \n",
      "        Examples:\n",
      "            >>> train_dataset, test_dataset = torchtext.datasets.YelpReviewPolarity(ngrams=3)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['LanguageModelingDataset', 'SNLI', 'MultiNLI', 'XNLI', 'SST...\n",
      "\n",
      "FILE\n",
      "    c:\\programdata\\anaconda3\\lib\\site-packages\\torchtext\\datasets\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torchtext.datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
